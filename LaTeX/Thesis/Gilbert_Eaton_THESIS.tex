\documentclass[fleqn,twoside]{article}
\usepackage[utf8]{inputenc}

%margins and size of page
\usepackage[twoside, top=3.0cm,bottom=3.0cm,right=2.5cm,left=2.5cm]{geometry}
\setlength{\oddsidemargin}{0mm} 
\setlength{\evensidemargin}{0mm} 

%1.5 line spacing
\renewcommand{\baselinestretch}{1.5} 

%dummy text
\usepackage{lipsum}

%for changing text sizes
\usepackage{mathptmx}
\usepackage{anyfontsize}
\usepackage{t1enc}

%changing space under headings
\usepackage{titlesec}

%figure placement
\usepackage{float}% If comment this, figure moves to Page 2

%table manipulation
%\usepackage{supertabular}
\usepackage{tabularx}
%\usepackage{tabulary}
\usepackage{longtable}
%\usepackage{ltablex}
\usepackage{booktabs}
%\usepackage{rotating}
%\usepackage{array}

\newcolumntype{C}{>{\centering\arraybackslash}X} % centered version of "X" type


%roman numerals
\usepackage{enumerate}% http://ctan.org/pkg/enumerate

%math tools
\usepackage[cmex10]{amsmath}
\usepackage[cmex10]{amsmath,mathtools}
\usepackage{fixltx2e}
\setlength{\mathindent}{0pt} %left align

%discrete math fonts
\usepackage{amsfonts}


%add programming code package
\usepackage{listings}

%\titlespacing*{<command>}{<left>}{<before-sep>}{<after-sep>}
\titlespacing*{\section}    %main heading
{0pt}{5.5ex plus 1ex minus .2ex}{0ex plus .2ex}
\titlespacing*{\subsection}    %subheading
{0pt}{5.5ex plus 1ex minus .2ex}{0ex plus .2ex}
\titlespacing*{\subsubsection}    %subsubheading
{0pt}{5.5ex plus 1ex minus .2ex}{0ex plus .2ex}

%subsubsubsection 
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

%nomenclature
\usepackage{nomencl}
\makenomenclature

%images
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{pdfpages}


%symbols
\usepackage{amssymb}


%for subfigures - side by side figures
\usepackage{caption}
\usepackage{subcaption}

%wrapped figures
\usepackage{wrapfig}

%header info
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\fancyhead[LO,LE]{} %remove automatic left headers
%\rhead{6007ENG - Industry Affiliate Program, Semester 1, 2015}

%footer line
%\renewcommand{\footrulewidth}{0.4pt}% default is 0pt

%paragraph - no indent with space
\usepackage[parfill]{parskip}

%bibliography
%\usepackage{biblatex}
\usepackage[square, numbers, comma, sort&compress]{natbib}

%appendix
\usepackage[toc,page]{appendix}


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
\usepackage{hyperref}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%stop space between paragraphs
\raggedbottom

\begin{titlepage}

% Declare new goemetry for the title page only.
\newgeometry{top=4.0cm,bottom=1cm,right=2.5cm,left=2.5cm}
%---------------------------------------------

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
 
\begin{flushleft} 
 

%----------------------------------------------------------------------------------------
%	Title section
%----------------------------------------------------------------------------------------


{ \Huge \bfseries Machine vision approach to identifying and grading Strawberries}\\[1.5cm] % Title of your document

%----------------------------------------------------------------------------------------
%	Name section
%----------------------------------------------------------------------------------------

\textsc{\Large \bfseries Mr. Gilbert Eaton --- B.Eng (Hons. I), B.IT}\\[0.5cm] %name
 
 
 
\vspace{10mm} 


%----------------------------------------------------------------------------------------
%	Supervisor information SECTION
%----------------------------------------------------------------------------------------

\textsc{\Large \bfseries Magnificent Pty. Ltd.}\\[0.5cm] % Name of school
\textsc{\Large \bfseries Griffith University}\\[0.5cm] % Name of uni
\textsc{\Large \bfseries School of Engineering - Griffith Sciences}\\[1.5cm] % course title

%----------------------------------------------------------------------------------------
%	Disclaimer SECTION
%----------------------------------------------------------------------------------------

\emph{A report submitted in partial fulfilment of the degree of Doctor of Philosiphy, and in confidence due to the agreement with ARC Linkage partners}\\[1.5cm]


%----------------------------------------------------------------------------------------


\end{flushleft}

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

% Ends the declared geometry for the titlepage
\restoregeometry
%--------------------------

%adds roman numerals to the TOC
\pagenumbering{roman}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	Abstract
%----------------------------------------------------------------------------------------


\section*{Abstract}

%adds unnumbered section to TOC
\addcontentsline{toc}{section}{ABSTRACT}



\vspace*{\fill}%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	Statement
%----------------------------------------------------------------------------------------
\newpage

\section*{Statement of Originality}
\addcontentsline{toc}{section}{Statement of Originality}

This work has not previously been submitted for a degree or diploma in any university. To the
best of my knowledge and belief, the thesis contains no material previously published or writtenby another person except where due reference is made in the thesis itself.

\vspace{50pt}

signed
Gilbert Eaton

\vspace*{\fill}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	Acknowledgements
%----------------------------------------------------------------------------------------
\newpage

% use section* for acknowledgement
\section*{Acknowledgements}

%adds unnumbered section to TOC
\addcontentsline{toc}{section}{Acknowledgements}

The author would like to acknowledge Griffith University staff Rudi Bartels, Dr. Andrew Busch and Prof. Yongsheng Gao for their guidance and knowledgeable advise during the project. Thanks also go to William Sheng who has been working on the project and provided invaluable knowledge.

....

\vspace*{\fill}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	Publications
%----------------------------------------------------------------------------------------
\newpage

% use section* for acknowledgement
\section*{Publications}
%adds unnumbered section to TOC
\addcontentsline{toc}{section}{Publications}


\subsection{First-Author Publications}

\begin{itemize}
	\item{G. Eaton, A. Busch, R. Bartels, Y. Gao, “A Method To Create Stable Lighting And Remove Specular Reflections for Vision Systems”,Digital Image Computing: Techniques and Applications (DICTA), DOI: 10.1109/DICTA.2017.8227392 (2017)}
	\item{G. Eaton, A. Busch, R. Bartels, Y. Gao, “Colour Analysis of Strawberries on a Real Time Production Line”, Digital Image Computing: Techniques and Applications (DICTA), (2018)}
\end{itemize}  



\vspace*{\fill}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	table of contents/figs/tables/nomenclature
%----------------------------------------------------------------------------------------

%1.0 line spacing
\renewcommand{\baselinestretch}{1.0} 
%contents
\newpage
\tableofcontents

%figures
\newpage
\listoffigures

%tables
\newpage
\listoftables

%nomenclature
%Strawberry jargon
\nomenclature{$cultivar$}{The species or type of fruit}%
\nomenclature{$punnet$}{A plastic strawberry container}%
\nomenclature{$calyx$}{The green leaves of a strawberry}%
\nomenclature{$peduncle$}{Stem of the strawberry}%
\nomenclature{$downtime$}{The amount of time (cumulative) during production where nothing is produced}%

%biological
\nomenclature{$SSC$}{Soluable solid content}%

%programming
\nomenclature{$API$}{Application Programming Interface}%

%cvip jargon
\nomenclature{$RMSE$}{Root Mean Squared Error}%
\nomenclature{$SVM$}{Support Vector Machine}%
\nomenclature{$RBF$}{Radial Basis Function}%
\nomenclature{$AI$}{Artificial Intelligence}%
\nomenclature{$BP$}{Back Propogation}%
\nomenclature{$FC$}{Fully Connected (NN layer)}%
\nomenclature{$ReLU$}{Rectified Linear Unit}%
\nomenclature{$NN$}{Neural Network}%
\nomenclature{$ANN$}{Artificial Neural Network}%
\nomenclature{$FNN$}{Feedforward Neural Network}%
\nomenclature{$CNN$}{Convolutional Neural Network}%
\nomenclature{$R-CNN$}{Region-based Convolutional Neural Network}%
\nomenclature{$BPNN$}{Back-propogation Neural Network}%
\nomenclature{$k-NN$}{k-Nearest Neighbour}%
\nomenclature{$DNN$}{Deep Neural Network}%
\nomenclature{$ELM$}{Extreme Learning Machine}%
\nomenclature{$SFLN$}{Single Hiden Layer Feed Neural Network}%
\nomenclature{$LDA$}{Linear Discriminant Analysis}
\nomenclature{$QDA$}{Quadratic Discriminant Analysis}
\nomenclature{$PLS-DA$}{Partial Least-squares Discrininant Analysis}
\nomenclature{$PCA$}{Principal Component Analysis}
\nomenclature{$MLP$}{Multi-layer Perceptron}%
\nomenclature{$GMM$}{Gaussian Mixture Model}%
\nomenclature{$PCA$}{Principal Component Analysis}%
\nomenclature{$IR$}{Infrared}%
\nomenclature{$RGB$}{Red, Green, Blue}%
\nomenclature{$HSI$}{Hue, Saturation, Intensity}%
\nomenclature{$HSV$}{Hue, Saturation, Value}%
\nomenclature{$CIE-Lab$}{CIE standard colourspace}%

%Spectral
\nomenclature{$NIR$}{Near Infrared}%
\nomenclature{$SWIR$}{Short-wavelenght Infraredrange of electromagnectic spectrum}%
\nomenclature{$MWIR$}{Medium-wavelenght Infraredrange of electromagnectic spectrum}%
\nomenclature{$LWIR$}{Long-wavelength Infrared range of electromagnectic spectrum}%
\nomenclature{$FIR$}{Far Infrared range of electromagnectic spectrum}%
\nomenclature{$UV$}{Ultraviolet range of electromagnectic spectrum}%


%computer jargon
\nomenclature{$UI$}{User Interface}%
\nomenclature{$PSU$}{Power Supply Unit}%
\nomenclature{$UPS$}{Uninterruptable Power Supply}%
\nomenclature{$CPU$}{Central Processing Unit}%
\nomenclature{$GPU$}{Graphical Processing Unit}%
\nomenclature{$RAM$}{Random Access Memory}%
\nomenclature{$PCIe$}{Peripheral Component Interconnect express}%
\nomenclature{$AVX$}{Advanced Vector Extensions}%

%photography jargon
\nomenclature{$FOV$}{Field of view}
\nomenclature{$LED$}{Light emitting diode (Light source)}
\nomenclature{$CCD$}{Charge coupled device (sensor)}
\nomenclature{$CMOS$}{Complimentary metal-oxide semiconductor (sensor)}

%mechanical
\nomenclature{$DOF$}{Degrees of Freedom}


%Sequence to make nomenclature refresh is:
%1 - Compile
%2 - Command - use terminal for the following:
%		C:\\>makeindex -s nomencl.ist -o Gilbert_Eaton_PHD.nls Gilbert_Eaton_PHD.nlo
%3 - Compile
\renewcommand{\baselinestretch}{0.5}

\setlength{\nomitemsep}{-\parsep}
 
\newpage
\printnomenclature[5cm]

%clearpage for page numbering 
\clearpage

%1.5 line spacing
\renewcommand{\baselinestretch}{1.5} 

%footer change
%\fancyfoot[CO, CE]{}
%\fancyfoot[RO] {\thepage}
%\fancyfoot[LO] {Gilbert Eaton}
%\fancyfoot[RE] {Quality Checking Strawberries using Multi-spectral Imaging}
%\fancyfoot[LE] {\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%changes back to numeric page numbering
\pagenumbering{arabic}
\newpage

\section{Introduction}

Machine vision systems have been successfully deployed to perform many tasks in industries such as manufacturing, automotive, mining, and is constantly branching into more diverse fields, for example, vehicle guidance \cite{menze,urmson}, drone applications \cite{greene,boucher}, traffic monitoring \cite{cheung,kamijo}, high-speed vision \cite{watanabe,nakabo}, and quality inspection tasks \cite{cubero, du}. This is due, largely, to recent technology advances in computational devices (CPU/GPU), storage and cloud services, and camera technology and affordability. These factors help in the adoption of machine vision for production quality control in order to provide multiple benefits over human inspection which may include labour cost reduction, consistent objective analysis, speed increases, safety risk reduction, and continuous services. Avoiding costly stock returns is an obvious benefit, however many costs can be unquantifiable or unclear such as customer satisfaction, and brand reputation. Production line quality inspection is one of the most common applications for machine vision as it is usually fixed in position with a known area to assess, giving the cameras opportunity to inspect each and every item produced. The cameras are also capable of working in environments that human operators may not tend to be comfortable in. Where the production lines may be exposed to heat, cold, dust, chemical, pressure, or noise hazards, cameras can solve the problem of human risk. 

Vision systems can be used for high-speed moving objects in order to perform the inspections where a human would have an impossible task. For example Duan et al \cite{duan} used a system of two cameras and two computers to inspect glass beer bottles after production at a rate of $500/min$ or $8.3/s$. They inspected the bottom of the bottle, the overall finish, and the top opening, performign all three checks for each bottle within the timeframe specified. After inspection, the defected bottles are ejected from the production line in order to be recycled. Elmasry et al \cite{elmasry1} developed a potato grading system which can acquire multiple images of each potato, from multiple sides due to the roller-conveyor, before performing Fourier transform, and Fourier shape descriptors. The conveyor speed is $1m/s$ and the spacing between each roller is $80mm$ giving approximately $12 potatos/s$ if there are no empty rollers. Four different grading/processing techniques were investigated by Kondo \cite{kondo} where, firstly, oranges were inspected by a system of cameras including six colour cameras at different positions (so that all sides are inspected), an NIR camera and the option of adding x-ray imaging to the purpose-built line. The oranges are fed through the system using a singulating conveyor which performs a $180^{\circ}$ rotation for the last camera to ensure all sides are evaluated. Secondly, an eggplant quality assemsment system was developed which made use of six colour and four monochrome cameras in order to determine colour, size, shape, and defects as well as a novel gloss detection method using angles cameras and three white strip lights to inspect the specular reflections sharp edges, meaning good quality, or dull edges, meaning poor. The 6 production lines, running at $38.1m/min$, at the facility meant that $504,000$ fruits could be processed per day. Even though there has been a substantial reduction in number of employees in processing, it is still laborious due to the packing requirements. The third project was capable of grading $10,000$ leeks per hour at a line speed of $30m/min$ with automatic root cutting and peeling steps. Lastly, a robot capable of grading $3 fruit/sec$ for 11 varieties of deciduous fruits including apple, pear, and peach. Using 3 DOF manupulators, the robot placed into trays, whilst images are acquired from top, bottom and two sides before making a grade decision. This shows that for fast moving production lines, humans are unable to peform the necessary tasks to quality inspect each item. The high speed vision systems are tailored to suit each application and are dependant on good lighting due to the rapid shutter speeds of the camera.




\begin{figure}[ht]
	\centering
	\includegraphics[width=300pt]{images/machine_vision.png}
	\caption{Example of production line using multi-spectral acquisition system with a robotic arm sorting the objects.}
	\label{fig:machine_vision}
\end{figure}

Most production line vision systems have a control mechanism that removes any defected items and allows only acceptable quality to pass. A basic diagram of this process is shown in Figure \ref{fig:machine_vision} whereby a camera provides information to the PC to decide which objects to accept and reject, while a mechanical device removes unacceptable items. This type of system configuration is illustrative of the basic concept of the process, whereas in real applications, the components and peripherals will be much more complex. Vision systems are designed so that the best possible images may be acquired without hindrance from external sources, and is usually very specific to the environment around the cameras, or scene. This may involve specialized lighting (illumination/spectrum), high-power lighting, camera type (high-speed, hyper-spectral, etc), object sensors, software, visualizations, and many different types of ejection/removal systems.



\subsection{Industry Partner}

Magnificent Pty. Ltd. is a subsidiary of Berry Yummy Marketing Pty. Ltd. - a strawberry farming and processing company located in Wamuran, Queensland. In this facility (as well as the company's secondary strawberry farm in South Australia), the company plants, grows, maintains, picks, and packs the berries ready for consumer purchase. Berry Yummy Marketing has operated the 100$ha$ farm for over 20 years in Wamuran, and has recently acquired a new, smaller 26$ha$ farm in Myponga, SA. Magnificent employs around 250 people in the peak of the season and consists of planting, picking, packing, driving (tractors/trucks) and operator teams. Producing around 1.7 million $kg$ of strawberries each year, Berry Yummy has a good market share with many high-profile customers including major supermarket chains such as Coles, IGA, and CostCo. The company has it's own transportation and storage facilities allowing for a distribution centre in Brisbane, Queensland's capital city. 

The Strawberries are picked and packed by hand using labour intensive methods in order to produce the finished goods, which can be sold in many different sized packages from 250g through to 3kg jumbo pack. The strawberries are packaged according to the quality grade or cultivar, however, if the fruit is in high demand or weather conditions cause shortages in supply, the lower grades can be used as high grade berries making the classification depend on market conditions.

\begin{figure}[h]
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.6\linewidth]{sunray.jpg}
		\caption{}
		\label{fig:sunray}
	\end{subfigure}%
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{whatwedo_logo.jpg}
		\caption{}
		\label{fig:whatwedo}
	\end{subfigure}%
	\caption{(a) Magnificent logo, (b) Workers and fields at Magnificent}
	\label{fig:test}
\end{figure}



Due to environmental and market demands, the standard of strawberries packed can vary between seasons, and as Magnificent employs a mostly casual workforce, the standard can differ between days also. Each operator must pack the correct weight, into the correct container, as fast as possible, whilst ensuring that there are no defected berries from a range of attributes such as over/under ripe, bruising, foreign objects, pest damage, dirt and soil, size, shape and overall appearance. This evaluation can be difficult in this fast-paced environment and could be easily overlooked in situations under pressure, especially when concerned with bruising. Bruising can be hard to detect, even for the trained eye, as the colour and texture takes some time to decay to an unacceptable level. The time taken to decay could be several hours to a few days and, as there may be up to 5 days transport time for berries picked by Berry Yummy, this leads to the case where finished product has left the distribution centre quality assured, but is rejected when it arrives at the destination. Berry Yummy are regularly given feedback from their customers regarding the quality of each punnet after it reaches it's destination point anywhere throughout Australia. Whilst training and education regarding these quality specifications is given, this will be lost as soon as the workforce is replaced by a new season of employees.  

In order to reduce costs and improve efficiencies, Magnificent has invested in various projects to bring a technological approach to farming. Some of these include a strawberry harvesting robot which must navigate through the fields many rows of planting mounds, picking strawberries \cite{busch}. A greenhouse picking system was also developed, where specialised planting troughs were constructed to bring the strawberries to the harvester using a vertical farming method. The company has also connected it's information systems together using a variety of applications interfaced to hardware such as weigh scales and employee clock-in stations. This gives better visibility to the managers and operators to make decisions in a real-time environment. They now have a requirement for an in-line strawberry quality vision inspection system which is capable of inspecting each strawberry in a fast-paced environment.



\subsection{Project Description and Requirements}
\label{sec:requirements}

\textbf{Research Question:} Can packaged strawberries be effectively and accurately quality graded by machine vision on a fast-paced, real time production line?

The concept of this thesis is to develop a system that is capable of assessing pre-packed punnets of strawberries for quality factors such as ripeness, size, rot and mould, bruising, and contaminates. The system must be able to decide between good quality and bad quality, before removing the punnets from the production line for further inspection or discarding. The production line, in it's existing operational capacity, can package two punnets per second, fed from multiple packing stations. The vision system must be designed so that it can be inserted into the current configuration, before the punnets are sealed and palletized, by either replacing existing conveyors or slightly extending the whole production line. 

The project that this thesis describes was conceived by Magnificent in order to minimize the risk of quality rejections and enhance the fruit's overall appearance on the shelves, and to overcome the aforementioned obstacles.

Coles is a major nationwide supermarket chain which has a 33\% market share in the fruit and vegetable retail sector \cite{roymorgan}. Therefore, the quality control of incoming goods are strictly adhered to by personnel and if the inspection of individual punnets fail, then the shipment may be at risk of being rejected and turned back to the distribution centre. This return could be as much as 4-5 days transportation and will, in most cases, be unsaleable causing financial loss.

In order to minimize this risk, the quality control of the strawberries must be improved and strengthened by adding a secondary quality control process, utilizing computer vision and image processing methods. In this way, the packers will be trained as usual, however, their finished punnets will now be assessed by the Strawberry Quality Assurance (SQA) vision system.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{punnet_flow.png}
	\caption{Block diagram of the proposed punnet flow.}
	\label{fig:punnet_flow}
\end{figure}

Since 2015, the strawberry packing line has been fitted with a machine that seals the lids on the punnets with plastic film instead of using a resealable lid. This measure has been adopted for a few reasons, namely, cost of packaging, ease of packing, and anti-tampering properties. Shown in Figure \ref{fig:heat_seal}, the heat-seal machine has a maximum speed of $120 punnets/min$ or $2 punnets/s$ and seals six punnets per cycle which improves the overall efficiency. Given that the machine must press down onto the stationary punnets with the heating elements, increasing the amount that can be sealed at once, increases throughput rate. However, the length of the heat-seal footprint is correlated to the number of punnets sealed per cycle. As there is limited space, the amount which can be processed simultaneously is also limited. The infeed conveyors must account for this batching of punnets by stopping when the sealing compartment is loaded, and starting when unloaded. The stop-start action is surpressed by an accumulator conveyor running at a slower speed and gradually feeding into the baching section, but still requires the infeed to stop, albeit a lesser frequency. This means when integrating production conveyors to feed this machine, the discontinuous nature of the flow of punnets must be considered.



\begin{figure}[ht!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{heat_seal.jpg}
		\caption{}
		\label{fig:heat_seal}
	\end{subfigure}%
	
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.6\linewidth]{unsealed_punnets.jpg}
		\caption{}
		\label{fig:unsealed_punnets}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.5\linewidth, angle=90]{sealed_punnets.jpg}
		\caption{}
		\label{fig:sealed_punnets}
	\end{subfigure}%
	\caption{(a) Heat-Seal machine, (b) Punnets before sealing, (c) Punnets after heat-sealing.}
	\label{fig:HS}
\end{figure}



Thin film rolls are used to supply the heating dye with materal to press and seal around the edge of the punnet, discarding the excess onto another roll. The pre-sealed and post-sealed punnets are shown in Figures \ref{fig:unsealed_punnets} and \ref{fig:sealed_punnets} and present a unique method to tamper-proof each punnet. The film, once removed, cannot be re-sealed giving consumers the confidence that each punnet has not been opened since it was packed. After being sealed, each item is passed through a metal detector, and a weight-checking scale. At both of these checkpoints, the punnets can be ejected from the production line using pneumatic air actuators.

The addition of a vision system to the heat-seal infeed is the desired outcome for Magnificent to address quality issues that can be missed in such a fast-paced environment. Defects such as underripe, overripe, bruised, rotten, foreign object and size and shape have all been attributed to stock rejects and/or returns. Sometimes it costs more to salvage rejected stock than reuse it, so there is a possibility that poor quality could mean a shipment is subject to dumping. As millions of full punnnets are packed, palletised and shipped each year the Quality Assurance Manager has an enormous, if not imposible, task in inspecting each one thouroughly. Strawberries are required to be picked and packed to a strict set of attributes. However, subjectivity, tiredness, rushing, overheating, can all influence the quality of berries picked in the field or packed on a production line. 

Subjectivity is of great concern due to the high turnover of staff, given the majority of employees are seasonal workers, only a few of which will return each year. Therefore, the vision system will inspect each punnet from the top and bottom, at high speed, and adding objective consistency to the process as one of the final quality steps before packing.


The requirements include that the vision system feed directly into the heat-seal machine, and not inhibit the production speed or volume. This is a critical measure, particularly during development and production line integration, which ensures that efficiencies are not lost in packing. The speed of packing punnets is directly related to the profit of both the company and its workers, both of whose payments are proportional to volume over time. The company loses money paying for workers to wait for machinery to be ready, hence, downtime due to a non-critical plant will be counter-productive and should be avoided. The inspection system must be able to detect the incoming, high-speed punnets and acquire images from above and below, before analysing the quality. If the system detects defects in the strawberry images, the punnet must be removed from the production line. 



The vision system general requirements are listed in Table \ref{tab:requirements} showing other critical success factors.



\renewcommand{\arraystretch}{0.8}% Tighter table height
\begin{longtable}{p{2.5cm}p{12cm}}
	\caption{Table of Major Requirements for the Strawberry Vision System}
	\label{tab:requirements} \\
	
	\hline\hline
	\textbf{Requirement} & \textbf{Details} \\[2pt]
	\hline \hline
	\textbf{\textit{Continuity}} 	& - Must not inhibit the production line speed. \\[2pt]
	& - The production line should not be caused to stop unnecessarily due to the vision system.\\[2pt]
	\hline
	\textbf{\textit{Safety}}    & - The vision system shall not create any safety risks or hazards such as tipping, electrocution, exposed pinch points, sharp edges, hazardous lighting, unsustainable lifting, etc.  \\[2pt]
	\hline
	\textbf{\textit{Inspection}} 	& - Autonomous operation.\\[2pt]
	& - The punnets are to be assesed based on the agreed defect categories (classes) to the specifications provided. \\[2pt]
	& - Each punnet will be evaluated from top and bottom (through the plastic container). \\[2pt]
	& - The infeed and outfeed systems integrate with the proceeding and preceeding machinery. \\[2pt]
	\hline
	\textbf{\textit{Footprint}}	& - Shall be externally connected to either one $400VAC$ or $240VAC$ power source, and one pnuematic line for all operations including removal from production line.  \\[2pt]
	& - Adaptability to multiple production lines. \\[2pt]
	& - Size of the system shall be kept as compact as possible due to packing room constraints. \\[2pt]
	\hline
	\textbf{\textit{Outputs}}	& - Defective punnets will be ejected from the production line.  \\[2pt]
	& - Operational statistics will be presented to help the operator make decisions based on performance.  \\[2pt]
	& - Light and sounds shall accompany the system to be intuitive to operators \\[2pt]
	& - Error, success, and general information shall be presented on a screen, or given to operators via another medium.  \\[2pt]
	\hline
	\textbf{\textit{Traceability}}		& - Records to be kept with each punnet's images as to when (date/time) they were packed.  \\[4pt]
	& - Images, timestamp, assessment, system state, and other records must be kept in a database for future access. \\[2pt]
	\hline
	\textbf{\textit{Documentation}}	& - Each software module is required be documented to describe the module and it's function and relevance to the application.  \\[4pt]
	& - A description of how the software is compiled and which dependencies are required to run the application will be provided. \\[2pt]
	\hline
\end{longtable}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Current Industry Methods and Review of Literature}
\label{sec:lit}

Numerous production lines have adopted computer vision systems in order to reduce the cost of labor and standardise the quality control. Depending on volume and product, quaity controllers may be a substantial cost in production and can be subjective, particularly for seasonal work. This leads to loss in productivity, labour costs, and financial loss in the form of stock returns, in the case that it is not accepted by the customer. Employing accurate vision systems on a production line give the producers quality control that is consistent, continuous, and reletively cheap. 

Performing a review on fruit Cubero et al \cite{cubero} investigated Apple, banana, citrus, cucumber, mango, mushroom, olives. potatoes, starfruit and watermelon as part of their investigation into agricultural machine vision. Methods such as colour analysis, histogram, k-NN, PCA, Fourier analysis, clustering, AdaBoost, and ANN's to perform the tasks of inspecting a wide range of defects and classifications. Another review by Dubey and Jalal \cite{dubey} investigated two types of tasks for vision systems - classification and defect detection - and concluded that most of the work in this field contained three steps in performing either task, namely, 1. Background subtraction 2. Feature extraction and 3. Training and classification. SVM, BPNN, Decision Tree, ANN, and even fusion classifiers were commonly used in their research.

As the field of production line machine vision in general has a wide spectrum of applications, the following literature review will only concern the relationship between produce grading and machine vision. 


\subsection{Image Processing in Produce Grading}

 
The use of simple techniques have been implemented in produce grading due to the inherant properties such as colour (ripening, defects), and texture (disease, freshness) extraction and analysis. Significant findings in experiments have led to vision systems adopting simple methods such as morphology, thresholding, fuzzy logic, colour space transformations, and edge detection (or a combination) to successfully detect or classify produce to an acceptable level or, in some cases, improve on more sophisticated processes.

Common colourspace transformations for colour grading and defect detection include HSV and CIE-Lab (Figure \ref{fig:colour-space})due to their specialised colour channels and properties. Work sampled by Pathare, Opara, and Al-Said \cite{pathare} in their review of colour measurment and analysis performed in order to evaluate fresh foods. They reviewed items including red table grape, tomato, orange, apple, banana, chicken breast and varied meat, flour, pasta, cerals, and breads all of which used a colour index (CI) to perform quality checks. They all had the commonality in that they used mathematical operations with co-efficients relating to one or more colour spaces such as HSV, CIE-Lab, and CIE-XYZ. These colour indexes were used to make a comparison against test samples for characteristic representation of maturation, preservation, or storage. 

\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{hue_sat.png}
		\caption{}
		\label{fig:HSV}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{CIELab-colour-space.png}
		\caption{}
		\label{fig:Lab}
	\end{subfigure}%
	\caption{Colour spaces (a) HSV and (b) CIE-Lab.}
	\label{fig:colour-space}
\end{figure}

Two species of table grape were graded into five categories by Cavallo et al \cite{cavallo} after training a Random Forrest Classifier with features from the CIE-Lab colour space. The team calculated the mean and standard deviation of each channel and added the product of channels and ratio of channels to achieve a $92\%$ and $100\%$ cross-validation accuracies for Victoria and Italia strains, respectively.

A method to grade lemons based on colour and volume was implemented by Khojastehnazhand, Omid, and Tabatabaeefar \cite{khojastehnazhand} where the system could be calibrated to the specific requirements before assessment begins. By passing through fruit as examples to the system (and stored in a database), before using the example data as comparrison to the unseen fruit. The colour and volume information was calculated via HSI colourspace and simple geometry measurements where the volume estimation $R^2 = 0.9852$ and colour (ripeness) estimated accuracies of $95.45\%$, $100\%$, and $86.67\%$ for classes 1, 2, and 3, respectively.

For many leafy vegetables green is the predominant colour, therefore, extraction of the healthy components is made easy using the abovementioned colour spaces. In order to identify leafy vegetables Danti, Magdi, and Anami \cite{danti} utilised the RGB and HSI colour space channels by simply finding the mean and range (variance was eliminated as a non-determining factor) of each channel as features. The features were used to train a BPNN model which scored in the range of $92-100\%$ for all 10 leafy vegetables including dill, fenugreek, corriander, spinach and mint. 

RGB or a combination of RGB with other colour spaces also give significant results for certain applications. In order to grade palm oil fruit, May and Amaran \cite{may} simply analysed the RGB range of underripe, ripe and overripe fruit colour and determined accociated values correlating to the ripeness. Their fuzzy logic system achieved an overall accuracy of $86.67\%$ showing that this method is plausible for fruit ripeness determined by colour. A method to grade four different varieties of mango into four ripeness grades was implemented by Nandi, Tudu, and Koley \cite{nandi} by extracting colour features, and deriving other colour features at key points across length of the fruit. The features were then used with a simple Gausian Mixture Model (GMM) to achieve accuracies that rivaled expert humans. Blasco, Aleixos, and Molto \cite{blasco2} developed an algorithm that can detect 12 different types of common disease in citrus fruits. The method reduces the image to 32 colours before performing a region-growing method and could assess fruit from different batches, and even species, of citrus without adjustments. This was based on the assumption that unaffected fruit peel is relatively consistent, smooth, and blemish-free. An overall accuracy of $94\%$ was attained with some of the most devastating diseases scoring $100\%$ detection. 

Geometric calculations are a common approach to assessing shape, size, or volume as the scale and intestity are invariant which facillitates the background removal process, leaving a precise outline. This outline can then be precisely measured due to the fixed position of a calibrated camera. Properties such as minor and major axis can be used to calculate area (size) and lenth-to-width (elongation) of objects. Contour curvature, shape matrices, moments, and shape transforms such as Fourier and Wavelets can also be calculated from a well segmented image. 

Sandrina et al \cite{sandrina} implemented a method using geometric calculations to determine weight and shape of watermelons. Using logistic regression analysis they determined that good specimen shape could be well-described by an ellipsoid model with an $R^2$ of $0.97$. The weight of the watermelons was estimated by image analysis with an error of $2.42\%$. A review of assessment techniques for legume quality was conducted by Mahajan, Das, and Sardana \cite{mahajan} that found several species including lentils, soybeans, peas, chickpeas, beans, and honeylocust were assesed for their geometrical properties such as size and shape. Using a flatbed scanner for each of these projects, the methods consisted of simple thresholding and morphology to find the binary image before measuring. 


Fuzzy systems can offer generalisation and adaptability properties for grading based on feature simplification. A Fuzzy Inference System (FIS) takes raw data (crisp), transforms it into a simplified version (Fuzzification), and makes a prediction based on some knowledge, before transforming again (De-Fuzzification) back into a crisp value. Fuzzification methods may include bucketing, averaging, PCA, or windowing in order to reduce the dimensionality of data, usually giving the ability to assign simple rules to perfom the classification or detection.

Hasan and Monir \cite{hasan} developed a system to evaluate guava by use of a FIS. The fuzzy rules were based on functions of hue, saturation, and intensity of the images acquired of the fruit. Their method achieved an accuracy of $93.4\%$ which is an improvement over other classifiers such as naive bayes and multi-SVM. A method that used volumetric estimation in order to perform quality grading in fruit (apples, mango, orange, pomegranate, and strawberry) was designed by Jadhav, Singh, and Abhyankar \cite{jadhav}. Four cameras positioned at various angles and elevations were used to acquire the callibrated images of fruits before the volumetric calculations were peformed. This information is coupled with the colour values from the hue channel of HSV colour space to obtain a classification grade which is then passed to an FIS for final distribution into five grades from very poor to very good. All grades and all fruits achieved accuracies of $>98\%$. 

An adaptive neural-fuzzy inference system (ANFIS) has been developed by Zheng, Jiang, and Lu \cite{zheng} in order to detect bruising in Chinese bayberries as a function of Fractal Dimension (FD) and RGB values. The multiple input, single output Takagi–Sugeno fuzzy network consists of an input layer, fuzzification layer, rule layer, defuzzification layer, and output layer. The trained network could dicriminate between bruised and unbruised bayberries with an overall $90\%$ accuracy. Similarly, Jiang et al \cite{jiang} used an ANFIS to predict ascorbic acid retention in fresh-cut pinapples. They used inputs of surface area, storage temperature, and storage time in order to train the predictor with a RMSE of $7.88\%$ and and $R^2$ correlation coefficient of $0.95$.

Goel and Sehgar \cite{goel} developed a Fuzzy Rule Based Classification System (FRBCS) in order to grade tomatos into six ripeness stages. Images acquired from the open farm environment were preprocessed by taking averages of R, G and B, R-G and R/G before training and testing, with comparrisons against Naive Bayes, SVM, MLP, and Random Tree classifiers. Their method of FRBCS achieved an accuracy of $94.29\%$ which outperformed the others ($88.89\%$, $87.42\%$, $84.97\%$, and $84.05\%$, respectively).  




\subsection{Hand-selected Features}

The process of hand-selecting features is a common approach to help classifiers or networks to focus on the most important characteristics which determine class or defect. If the feature is a known conditional variable for the application then it can be a simple matter of extraction. For example, a particular fruit may have a distinctive disease indicator such as black spots or rough texture that can be exploited and visualised by applying image processing techniques. 

A tomato maturity level grading system developed by Wan et al \cite{wan} by projecting five concentric circles with different radii onto the fruit in order to extract the colour feature value. The correlated hue values were used to represent the maturity level before being trained on a 3-layer BPNN, achieving an average accuracy of $99.31\%$. Mebatsion, Paliwal, and Jayas \cite{mebatsion} found the best results when pairing morphological features with colour features in order to classify cereal grains into five common categories. They used a simple least-squared classifier obtaining a result of $98.5\%$ for barley, $99.97\%$ for CWRS (Canada Western Red Spring), $99.93\%$ for oat, and $100\%$ for rye and CWAD (Canada Western Amber Durum). Muhammad \cite{muhammad} also used hand-selected features when developing a method to classify date varieties. After an ellipse has been fit to the date image, features such as major and minor axis length, eccentricity, area, and texture descriptors were extracted and trained with an SVM classifier to achieve accuracies of $100\%$, $96.2\%$, $96.6\%$, and $99.6\%$ for the four varieties analysed. These results show that a small number of features, if properly selected, has the potential to perform extremely well under the right conditions.

Depending on the architechture used to classify the features, variance may be high especially when using neural networks. Features that best represent images for humans are not necessarily best represented for machine learning. Pereira et al \cite{pereira} hand crafted 21 colour features utilising three colour spaces (RGB, CIE-Lab, and HSV) in order to grade papayas into three maturity stages (MS). The mean values, pixel areas, and differential indexes between these colour spaces were obtained and classified by Random Forrest method resulting in an accuracy of $95.4\%$, $92.1\%$, and $84.4\%$ for MS1, MS2, and MS3, respectively. The team noted that it may be suitable for an industrial application in future. Szczypinski, Klepaczko, and Zapotoczny \cite{szczypinski} used an ANN classifer to attempt to discriminate between 11 different barley strains. 13 statistical metrics were obtained such as mean, skewness, second moment, and entropy in order to differentiate the varieties with an accuracy ranging $67\%$ to $86\%$. In an effort to grade persimmon fruit into three commercial maturity stages, Mohammadi, Kheiralipour, and Ghasemi-Varnamkhasti \cite{mohammadi} extracted features relating to RGB, CIE-Lab, HSV, and greyscale conversions before training both an LDA and QDA classifier to perform the categorization. The QDA model achieved an overall accuracy of $90.24\%$. Selected features chosen by Thendral and Suhasini \cite{thendral} improved the results, compared to no feature selection, for all tests performed whilst detecting defects in orange peel. They compared the performance of SVM, BPNN, and AANN finding that the AANN delivered the best testing results of $94.5\%$. Gill, Girdhar, and Singh \cite{gill} used a hybrid intelligent solution when developing a fruit grading method by means of a Genetic Algorithm merged with BP-ANN. The Back-Propagating (BP) method used to update weights in the network has a tendancy to become trapped in a local minima, leading to the idea of genetic algorithm (imitating natural evolution) coul overcome this problem. The results indicate that the hybrid system has increased accuracy ($93.33\%$) compared to a conventional BP-ANN ($73.33\%$).  



\subsection{Physical Property Analysis}

Hardware and physical properties may be exploited in some applications in order to avoid complex processing or improve results. Mutiple cameras, multiple spectrums and 3D imagery are examples of hardware employed in order to allow vision systems to capture details of the subject which may go unseen with conventional systems. 


Chui et al \cite{chiu} used a process of acquiring flourescence images - exiting the internal structure of the object in one wavelength, to emit light at a different wavelength - in order to detect mechanically-induced bruises on apples. They found that this method highlighted the bruised flesh, and extracted them by performing an image difference with a local adaptive binarisation method. 


Spectroscopy is a method of measuring the transmitted spectral signature of objects. By placing the light source on the opposite side, and not in direct view of the sensor, the transmittance can be measured to show unique properties regarding the internal structure of the subject. Figure \ref{fig:spectroscope} illustrates the fruit spectrocopy system from \cite{choi}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{spectroscope.png}
	\caption{Example of a spectroscope taking measurements from a fruit sample.}
	\label{fig:spectroscope}
\end{figure}%


Matteoli et al \cite{matteoli} used a fibre-optic spectrometer to take measurements of peach fruits whilst performing destructive density testing in order to make predictions on new fruit non-destructively. They compared the output of a crisp versus fuzzy logic approach and found overall average accuracies of $60\%$ and $~80\%$, respectively, making the fuzzy system appropriate for the task. Choi et al \cite{choi} also used an NIR spectrometer to assess the internal qualities of pear fruit as well as CCD cameras to inspect the visible characteristics such as skin blemishes and colour. The spectroscope analysed the fruit for sweetness, acidity, hardness, and moisture before combining the two (visible and internal) features as classification variables for an ANN. 

A hyperspectral range of $400nm-1000nm$ was used by ElMasry et al \cite{elmasry2} to inspect various fruit by it's moisture content(MC), soluble solids(TSS), and acidity(pH). The resulting corellation coefficients of both the Partial Least Squares ($0.90, 0.80, 0.87$, respectively) and Multiple Linear Regression ($0.87, 0.80, 0.92$, respectively) proved the feasability of predicting these physical characteristics. They also measured three ripeness classes based on grey-level co-occurrence matrix (GLCM) analysis achieving $89.61\%$ classification accuracy.


Beyond the Near (NIR), Short (SWIR), lies the Mid (MWIR) Wavelength Infrared, and  Long Wavelength infrared (LWIR) which is the part of the spectrum ($3\mu m-15\mu m$)that objects near room temperature emit thermal radiation. This thermal information can be detected by specialised sensors and converted to correlated visible colours for humans to view as seen in Figure \ref{fig:thermal} taken from \cite{baranowski}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{thermal.jpg}
	\caption{A thermal image apple with bruises showing the Long Wavelength Infrared (LWIR) part of the electromagnetic spectrum. In this range, sensors can detect thermal properties at room temperature}
	\label{fig:thermal}
\end{figure}%


Thermographic images in the wavelength range $3.4-5.2\mu m$ assisted Ginesu et al \cite{ginesu} to develop a thermal inspection system used to detect foreign bodies in produce items on a production line. Thermal imaging requires heaing or cooling in order to stimulate the subjects before being imaged sometime afterwards. Different materials heat transfer rates are varying, making the process of distinguishing, for example, sticks or stones from the edible items. They used a contrast enhancement and region-growing method to extract the highlighted regions from the greyscale thermal images.

On the opposite side of the visible spectrum resides ultraviolet, followed by X-ray wavelengths at which common medical and security inspections occur. X-rays are capable of penetrating objects in, for example, airport security allowing the users to visually inspect the contents of luggage. Medical applications include bone tissue and oegan imaging in order to help physicians make informed decisions about pateients. By choosing the optimal wavelength, many materials can be visualised internally including fruits and vegetables.    

Mathanker et al \cite{mathanker} analysed good and defective pecans using X-ray images. They compared AdaBooost and SVM classifiers finding that the AdaBoost average classification result of $92.2\%$ was the best performing.

A laser backscattering technique used by Lorente et al \cite{lorente} provided images that could determine the presence of pathogen Penicillium digitatum in oranges. They directed a laser beam onto the fruit and took images of the surface reflectance using five laser wavelengths ($532nm$, $660nm$, $785nm$, $830nm$, and $1060nm$) in order to perform experiments with the best results of $96.1\%$ attained by using all 5 wavelengths in combination trained with an LDA classifier.



\subsection{Hyperspecral/Multispectral Imaging}
\label{sec:lit_hyperspec}

As the spectral range of the eye is limited, camera sensors can be emeployed which can viualise wavelengths beyond these restrictions into the ultra-violet and infrared ranges. Figure\ref{fig:eye_sensor} shows the spectral response of the human eye versus CMOS and CCD sensors illustrating the extent to which the electronic sensor's range is greater.

\begin{figure}[h]
	\centering
	\begin{subfigure}{.9\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{spectralresponse.jpg}
		\caption{}
		\label{fig:eye_sensor}
	\end{subfigure}

	\begin{subfigure}{.9\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{infra-red.jpg}
		\caption{}
		\label{fig:ifra-red}
	\end{subfigure}%
	\caption{(a) Spectral response of human eye (left) and CCD and CMOS sensors (right). (b) Infrared}
	\label{fig:spectrum}
\end{figure}%

Other sensors exist whose range is further into the infrared (SWIR, MWIR, LWIR, and FIR in Figure \ref{fig:ifra-red}), allowing many properties (usually internal) to be analysed.

Classification can be performed using hyperspectral imaging as well as defect detection. Invisible properties of fruits and vegetables in particular, can be used to classify between species, or grades of a particular species. Munera et al \cite{munera} developed a system using visible and NIR hyperspectral imaging in order to grade persimmon fruit by firmness, maturity level, and astringency. Using CIE-Lab colour space, they found that the $L$ and $b$ coordinates decreased whilst the $a$ coordinate increased with maturity, allowing good corellation with colour parameters such as $H(R^2 = 0.83)$, $G(R^2 = 0.82)$ and $h(R^2 = 0.81)$. Ratios of the colour channels were also analysed with results achieving $a/b(R^2 = 0.83)$, $G/R(R^2 = 0.83)$, and $a/L(R^2 = 0.83)$. Three wavelengths were chosen as optimal ($580nm$, $680nm$, and $1050nm$) in order to perform the ripeness tests with all accuracy $>94\%$ using LDA, QDA, and SVM classifiers. The astringency test used the same classifiers and resulted in $>90\%$ for all, and $>95\%$ for the QDA method. This indicates the ability of non-destructive assessment of the maturity, firmness, and astringency of these fruits is valid. Another example by Munera et al \cite{munera2} acquired images with wavelengths of $450nm-1040nm$ for the purpose of distinguishing between two kinds of nectarine that have very similar appearance but different taste. A human panel could only classify correctly $57\%$ of the time, whereas their method had a $94\%$ accuracy using a PLS-DA model trained with 14 optimal wavelegths. 

In order to grade cherries into three maturity stages, Li et al \cite{li} analysed characteristics in terms of Soluable Solids Content (SSC) and acidity (pH) with hyperspectral imaging in the range of $874nm-1734nm$. Two full-band methods were tested, namely, principal components regression model and partial least squares regression model both with similar accuracy. A genetic algorithm (GA) was used to reduce complexity in bandwidth by reducing the full spectrum to 54 optimal bands for the SSC model and 28 for pH, and reported a $96.4\%$ total accuracy.  


Bruising in soft-skinned fruit and vegetables occurs if mishandled or exposed to excessive handling, reducing the quality for customers. Usually resulting from an impact, the moisture comes to the surface and therefore can be differentiated from the sourround soun tissue by means of analysing several water absorbtion bands in the visible and NIR regions. Therefore, NIR and SWIR wavelengths can penetrate the surface of some materials giving insight into hidden or skin defects ranging from sub-surface bruising to external visible tissue damage. Visualising these inpurities can be challenging for RGB imaging given inherrant colour saturation and reliance on good illumination. 

Using hyperspectral imaging ranging from $950nm$ to $1650nm$, Lee et al \cite{lee} exploited the ability of NIR wavelengths to detect bruising in pears. By investigation it was found that the waveband ratio of $1074nm/1016nm$ higlighted invisible bruises which could then be extracted using thresholding with a $92\%$ accuracy. 

Hyperspectral imaging and PCA reduction used by Che et al \cite{che} provided an accurate method to extract bruise regions from apple images. Using two characteristic wavebands at $675nm$ and $960 nm$, they analysed the fruit $0h$, $12h$, and $18h$ after bruising impact and found that the Random Forrest classifer performed best at $99.9\%$ average accuracy. Qiang and Mingjie \cite{qiang} used hyperspectral imaging  in the range of $408nm-1117nm$ in order to detect bruising in kiwi fruit. They were able to detect the bruising through the thick peel where human vision could not with an overall error rate of $14.5\%$. Li \cite{li3} also investigated detection of buising on peaches in the invisible NIR and SWIR range only ($781nm-2500nm$) in combination with and improved watershed segmentation technique resulting in a bruise detection accuracy of $96.5\%$ and sound sample detection accuracy of $97.5\%$.

Other species have similar bruise detection properties with infrared imaging such as apples \cite{che}, kiwi \cite{qiang}, and Pear \cite{lee}. Lopez-Maestresalas et al \cite{ainara} investigated two wavelength bands at Vis-NIR ($400nm-1000nm$), and SWIR ($1000nm-2500nm$)in order to detect blackspot in potatos which occurs on heavy impact during harvesting. The dark skin of the potatos inhibits visual detection for blackspot going largely undetected. Their analysis showed that, using HSI colour space and PLS-DA, they could achieve an overall classification accuracy of $>94\%$. This result was superior over the other methods examined, namely, PCA and Soft Independent Modeling of Class Analogy (SIMCA). They also found that the SWIR band possesed features which allowed early detection of the defect with $98.56\%$ accuracy $5h$ after impact.

A strawberry decay detection method designed by Liu et al \cite{liu2} used SWIR ($1000nm-2500nm$) imaging to determine attributes such as Total Water-soluble Sugar (TWSS), the sum of fructose, glucose, and sucrose finding several distinct bands to evaluate. Predicting the TWSS content resulted in $R^2=0.807$ correlation coefficient and the accuracy of the decay classifier generated between $89.4\%-95.4\%$ for calibration and $87.0\%-94.4\%$ for prediction accuracies, respectively.

Skin defects in bi-coloured peaches were successfuly detected by Li et al \cite{li2} by investigating wavelengths from $400nm$ to $1000nm$ determining characteristic values at $463nm$, $555nm$, $687nm$, $712nm$, $813nm$, and $970nm$ as well as $781nm$, $815nm$, and $848nm$. The best results of $96.6\%$ were realised when taking the ratio of $781nm/848nm$.

Strawberry fungal infection identification performed by Siedliska et al \cite{siedliska} analysed wavelengths in the range of $400nm-2400nm$ finally selecting 19 bands to discriminate infected vs sound berries training a BNN model. They achieved $>97\%$ accuracy for fungal infection detection from the $24h$ point, and using the same bands, performed a multiple linear regression technique to predict anthocyanin content (AC) and soluble solid content (SSC) with results of $R^2=0.65$, and $R^2=0.85$ coefficients, respectively.





\subsection{Strawberry Vision Systems}

Strawberries are a delicate fruit with an unprotected, soft, edible flesh, and very susceptible to damage if mistreated, handled too much, or even transported carelessly. The outer flesh of the berry is pinned with seeds creating higher surface friction, coupled with the incredibly soft exterior, creates challenges in terms of handling during picking, packing, and distribution. This is conversely true for fruits such as mango, watermelon, oranges, kiwi, and banana, for example, which have a protective, smooth layer of peel to protect the flesh inside, seen only by the consumer. Some success has been made in machine vision for strawberries, with examples in robotic picking, picking and grading, and grading on a production line. 

Brosnan and Sun \cite{brosnan} performed a review of agricultural vision systems including apple grading, orange stem detection and sweetness correlation, unsplit pistachio nuts, tomato size, colour, and shape properties as well as seedling health, peach and pear maturity levels, and automatic fruit harvesting. The team also reviewed vegetable inspection methods such as musshrooms and potatos, as well as wheat, rice and corn. The team noted 2 methods for analysing strawberry shape and size, taking one of the teams $1.18 s/berry$ and noting that fruit quality is dependant on a number of pre-harvest and post-harvest processes. 

Using a conveyor system, L. Xu et al \cite{xu} achieved very good results in classifying shape, ripeness, and size. The measurements are attained by using a K-means clustering method to find 7 vertical and 7 horizontal axis lines. Size feature was calculated by performing experiments to find the ratio of pixels/mm and simply dividing the pixel measurements of the berry by this ratio. Using the CIELAB colour space, a dominant colour was found in the berry by means of a histogram windowing method. Similarly, Liming and Yanchao \cite{liming} developed a strawberry grading method and mechanical system that requires manual loading of single strawberries on a converyor belt before using colour features and k-means clustering to grade the fruit into shape and colour classes at a rate of approximately $2.5s/berry$. 


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{strawberry_picker.png}
	\caption{Strawberry picking machine (a) demonstrating end-effector mechanism (b) placing strawberry into the foam tray.}
	\label{fig:strawberry_picker}
\end{figure}%


S. Yamamoto et al \cite{yamamoto2} used simple RGB techniques to define the ripeness of strawberries when developing an automated harvesting machine. They used a combination of green, red, and white LED's in order to illuminate the strawberries in specific wavelengths, giving better judgement on whether to pick or not based on ripeness. The harvesting machine was designed to be stationary with the fruit being presented to the machine via moving beds, which implies a large amount of fabrication and process chgange would be required to persue this method. The authors reported high harvesting rates ($67.1\%$) compared to that of other more conventional harvesters, however, their system was succeptible to fruit damage and requires improvement in harvesting speed. Hayashi et al \cite{hayashi} reported a rate of $11.5s/berry$ using their automated harvesting machine. The end-effector designed by the team used a peduncle cutter in conjunction with a suction cup to pick the fruit from the plant and place it in a padded tray as shown in Figure \ref{fig:strawberry_picker}. The HSI colour space was used to generate a maturity level when grading the fruit to be picked. By creating acceptable and unacceptable sections of the average hue, saturation, and intensity channels, they could discern $>80\%$ ripeness with an accuracy of $41.3\%$.


Strawberry cultivar are numerous around the world and their distinguishing features seem only obvious to expert humans as shown in K. Yamamoto et al \cite{yamamoto} when they implemented a method to distinguish between 21 different strawberry cultivars. They found that when using a single feature such as colour, shape, or size, the average accuracy was very poor at $<42\%$. However, when these three features were used in conjunction, the discriminant analysis classifier performed better at $68\%$. This shows that strawberry cultivars are not simply classified due to the inter-species colour and form having similar features. 


\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{multispec_strawberry.png}
	\caption{Pseudo-coloured strawberry heatmap showing the white area of the berry being firmest and the red area softest.}
	\label{fig:multispec_strawberry}
\end{figure}%

EMasry et al \cite{elmasry2} used multi-spectral imaging in order to determine the moisture, soluable solids, and pH levels in order to grade the ripeness of single strawberries. They found broadband absorbtion bands, in the case of underripe fruit, at 500, 680, 840, and $960nm$. The bands at 840 and $960nm$ were found to represent sugar and water absorbtion which was used to determine factors that would usually require destructive testing. The same results were found by Tallada, Nagata, and Kobayashi \cite{tallada} using an NIR hyperspectral system, they analysed and asessed the firmness of strawberries by non-destructive imaging. By taking acquiring images from $640nm-1000nm$ in steps of $5nm$ they were able to dertermine the optimal wavelengths to estimate firmness accross three grades of ripeness. The results had a correlation of $0.786$ and standard error of $0.350MPa$ using $685nm$, $865nm$, and $985nm$. They calculated formulas to generate heat maps of the firmness distribution (pseudo-colour), as shown on the ripe and underripe examples in Figure \ref{fig:multispec_strawberry}. This result is expected given the deep red colour of the top specimen versus the half white, and red example.

Zhang et al \cite{zhang} used a hyprspectral imaging system in two ranges - $441nm-1013nm$ and $941nm-1578nm$ to assess the ripeness of strawberries into three categories. Using PCA for reflectance processing and texture components at optimal wavelengths to form the feature set, they trained an SVM classifier achieving overall classification accuracy of $85\%$. Similar research by Liu et al \cite{liu} used 19 bands in the range of $405nm-970nm$ in order to deternine firmness, SSC, and ripeness. Experimenting with classifiers such as SVM and PLS, they found the BPNN model performed well with $R^2=0.94$ and $R^2=0.83$ coefficient of prediction for firmness and SSC, respectively. Three category ripeness evaluations were performed using both PCA-BPNN and SVM clasifiers with the SVM model achieving $100\%$.

 




\subsection{Real-time Systems}

One of the most challenging aspects of developing vision systems designed for quality inspection is the process of production implementation. Accuracy, speed, safety, power, robust design, operator use, control mechanisms, and signals must all be considered in order to progress from bench design to production-ready. Control mechanisms and speed of processing are the most prominent of these considerations, with many solutions using simplified processes.

Fruit and vegetables are generally graded by the ripeness or rediness to eat, based in predominance on their colour, size, and texture. Therefore many solutions to real-time grading use simple colour algorithms, many in combination with size and texture estimation. Sofu et al \cite{sofu} designed and automatic apple sorting system which processed at a rate of $54,000 apples/h$ acheiving an accuracy of $79\%$. A simple descision tree was used to classify the specimens according to colour, size, stain and weight before sorting the apples using opening bowl mechanisms. Their system, described in in Figure \ref{fig:apple_system}, shows the lighting box enclosure, operator control panel and PLC display, and control system of opening bowls in the foreground of the diagram.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{apple_system.png}
	\caption{Apple sorting system showing the lighting box with cameras in the background with opening bowls in the foreground.}
	\label{fig:apple_system}
\end{figure}%

A fig sorting system developed by Baigvand et al \cite{baigvand} could successfully categorise the figs into five grades with accuracies up to $95.2\%$, processing at a rate of $90kg/h$ and sorting each into separate bins using air nozzles. The team created indices related to colour, size, and the area of the split, seen on each fruit, applying a rule-based method to determine the graded category. 

In order to remove the green calyx from strawberries, Lin et al \cite{lin} used a waterjet cutting system guided by machine vision. The strawberries were fed into the system using roller rods to align and separate before using CIE-Lab colour space to segment the berries from the calyx. A series of geometric calculations guide the cutting system with the optimal cut line. Their machine was designed to process berries bound for ice cream, yoghurt, jucing, and jam markets instead of being packaged for sale as fruit, meaning that condition, bruising and ripeness was not of concern as much as removing the green leaves.

Hyperspectral and mutispectral implementations can help systems to assess products in real time due to their abitlity to see what the human eye, or regular cameras can detect. This advantage can provide information which simplifies the processing of images, for example, the use of NIR wavelengths can uncover invisible defects (as discussed in Section \ref{sec:lit_hyperspec}). Lee et al \cite{lee} implemented an industrial date grading machine capable of processing $72,000 dates/h$ with an $87\%$ accuracy measured. They acquired images in NIR wavelengths ranging $750-1200nm$ by use of an NIR-extended CCD camera and a high pass filter to block wavelengths lower than $750nm$. Processing at a rate of $20 fruit/s$ ,these wavelegths provided good background segmentation, added to the amplification of delaminated skin features which determine quality. The amount of delaminated skin was calculated, after morphological operations, and compared to the size of the entire fruit to give a percentage and a grade from 'A' to 'E'. 

When implementing systems in production environments, the rate of production (and limited space constraints) sometimes require inventive solutions to be implemented in order to acheive the desired outcomes. Aleixos et al \cite{aleixos} developed a commercial citrus grading system which used a combination of transmissive and reflective lenses in order to image the same scene with two cameras. One RGB and one NIR camera were used to acquire images of citrus fruits at a rate of $5 fruits/s$ before assessing each for size and colour and surface defects. The size estimation had $<2mm$ error with colour grading for oranges, lemons, and manderins reaching $94\%$, $93\%$, and $94\%$, respectively.

Machine vision can outperform humans depending on the task involved. Particularly for borderline cases a machine has finite thresholds inplace to ensure consistent outcomes. Estimating ripeness colour, areas of scattered blemishes, and measurements can be difficult and/or subjective between human experts but easily calculated with vision systems. A meat marbling grading system developed by Barbon et al \cite{barbon} trained a k-NN classifier to grade meat samples in $<1s$, which is an improvement on human gradation time of $11s$. Their method was suitable for a range of lighting conditions and animal type due to the illumination normalisation and contrast enhancement processes before classification attaining results of $81.59\%$ and $76.14\%$ for bovine and swine, respectively. 



\subsection{Machine Learning / Deep Learning Methods}


Machine learning is a branch of computer science and AI that uses an iterative 'learning' approach to building a classifier model. The model's weights are updated by an algorithm that, given any training set ($x_1, y_1$), ($x_2, y_2$), ... , ($x_n, y_n$), can learn a function $y = f(x)$ to make prediction of an unseen $x_{test}$ variable. Deep Learning refers to the depth of the network or number of layers the model has. Using a deeper network allows for multiple convolutions in a CNN and therefore better abstraction, albeit more computationally expensive. A range of methods have been investigated from very basic MLP networks, SVM, and ANN to more complex architectures such as CNNs.


Comparison of MLP neural network and Discriminant Analysis (DA) performed when grading golden delicious starfruit by Abdullah et al \cite{abdullah} found that the colour attributes were more accurately classified by DA, with resulting $95.3\%$ and $90.5\%$ for DA and MLP, respectively. The method used to extract colour features based on hue values $10-74$ were improved with the DA classifier using Wilks-lambda reduction, however the MLP result was unchanged. In order to classify shape characteristics, Fourier discriptors were used generating a $100\%$ accuracy.


Zhang et al \cite{zhang} used an FNN to classify a fruit dataset created via on-site collection and on line. After background removal, the 79 features that had been constructed based on colour, texture, and shape were obtained before being reduced via PCA to 14, and used to train the network using 5-fold cross validation. The classification accuracy of $89.1\%$ outperformed other methods tested such as BP, momentum BP, and GA. 

Even for simple networks, the relationship of problem domain to network architechture is not well defined in terms of accuracy. Zuniga et al \cite{zuniga} used a supervised learing approach with small ANNs in order to grade grape seeds from images acquired scattered over a flat-bed scanner. Using a process of analysis and trial and error, the optimal number of neurons is found and a classifier trained giving total accuracy of $86\%$ for the test set. An apple grading system developed by Vakilian and Massah \cite{vakilian} used a 3-layer ANN whilst grading Golden Delicious and Red Delicious varieties with respective overall accuracies of $89\%$ and $92\%$. Extracting the images textural (mean and variance of energy values) features before training separate networks for each apple variant. Finally, they invstigated the width of the ANN's middle layer ranging the architechture of the network from $2-2-4$ to $2-20-4$ finding the optimum width of $2-12-4$. 

Conversely, Wang et al \cite{wang} had perfect results using a similar architecture whilst developing a method to find worm-eaten holes in chestnuts. Using Sobel edge detection and using the resulting, filtered region to train a BP network. Conversion occured after only three iterations and achieved $100\%$ for both good and wormhole classes.

When using hyperspectral/multi-spectral data, it is possible to extract internal attributes such as structure, soluable solids (sugar content) and pH levels depending on the subject and wavelength range. Sugiyama et al \cite{sugiyama} were able to detect stems and leaves (foriegn objects) from hyperspectral blueberry images. They found that the foliage was easily extracted at particular wavelengths (1268 and $1317nm$) and were then able to perform a discriminant analysis between the two wavelengths in order to segment and highlight the defects.

Extreme Learning Machines (ELM) have gained popularity in the last few years. An ELM is used to train a single hiden layer feed neural network (SLFN) in a more efficient way by performing a random feature mapping and a least squared formula based linear regression \cite{peng}. Luo et al \cite{luo} used a kernel extreme learning machine to create a mulit-label classifier used in applications where designators are applied to an unseen object. Xu et al \cite{xu} used an ELM to continuously modify it's classifier in a dynamic process due to concept drift in applications. Extra nodes are added when an alert flag is set in the algorithm, and if the error rate drifts too far, the classifier is erased and a new classifier is trained. A novel twin ELM algorithm was used by Wan et al \cite{wan} to train two classifiers simultaneously by creating two non-parallel hyperplanes. 

ELM have proven to be an efficient and accurate method to train classifiers due to the least square formulation approach to find the solution, opposed to the first and second order gradient descent method \cite{yavs}. the ELM out-performs other classifiers such as SVM and k-NN which have been tested independently \cite{yavs, wan, peng}, due it's combination of fast training times, testing times, high accuracy, and low error rate. Several applications were able to implement ELM in real time systems due to these factors \cite{xu, yavs}.  

Mohammed et al \cite{mohammed} used a novel Bi-directional two Dimensional Principal Component Analysis (B2DPCA) in conjunction with an Extreme Learning Machine (ELM) in order to greatly improve facial recognition accuracy in popular face datasets. The B2DPCA technique proposed keeps the data in matrix form as opposed to vector form for PCA processing, and the ELM is a variation of a feed forward, single hidden layer neural network, with the main difference being assignment of random input weighs and hidden layer biases which transforms training the network into a linear system. Zheng, Fu, and Ying \cite{zheng} used an Extreme Learning Machine (ELM) to classify spectroscopic data acquired from four different foods - coffee, meat, oil ,and fruit. The results indicate good results for the coffee, meat, and oil with $100\%$, $97.78\%$, and $97.35\%$ accuracy, respectively, whilst the fruit classification was less accurate achieving $95.05\%$. They compared all the datasets and ELM by applying five different methods including KNN, DA, and ANN, finding that SVM was the most appropriate. 


\subsubsection{Support Vector Machines}


SVM can be considered a type of neural network that has only one hidden layer of neurons. The SVM attempts to create a hyperplane between the feature sets of each class. The support vectors are formed as non-zero coefficients of the data points found after processing the training set. If the margin is large between the hyperplane and each class, the system is said to be a stronger SVM classifier and should produce good results as explained by Osuna, Freund, and Girosi \cite{osuna}. Standard SVM inherently is incompatible with large datasets which suits applications with a small set of training examples. This is due to the $O(m^3)$ training time and $O(m^2)$ space complexity where $m$ is the size of the dataset \cite{tsang}.

Given a set of training data {$(x_i, y_i), x_i \in X^n, y_i \in (-1,1), i = 1,2,3,...,n$} th SVM kernel attempts to solve the hyperplane decision boundary equation:
\begin{equation}
w \cdot x + b = 0
\end{equation} 
giving
\begin{equation}
w \cdot x + b \geq 0, \quad \textrm{for} \quad y_i = +1
\end{equation}
and
\begin{equation}
w \cdot x + b < 0, \quad \textrm{for} \quad y_i = -1
\end{equation}

Here, $w$ constitutes the variable vector and $b$ the bias vector. Beginning with the kernel function $K(x, x^\prime)$ which obtains the non-linear mapping $\Phi(x)$, the solution takes the form:
\begin{equation}
f(x) = b + \sum_{i=1}^{m} y_i \alpha_i K(x, x^\prime)
\end{equation}
which is the summation of each support vector of length $m$, the respective $y$ values, Lagrangian multiplier ($\alpha_i$), and the kernel $K(x, x^\prime)$. Figure \ref{fig:SVM} shows an example of a hyprplane desicion boundary set by finding the solution above.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{SVM.png}
	\caption{A 2D example of SVM yperplane separating two classes. The examples on the boundary are considered the support vectors.}
	\label{fig:SVM}
\end{figure}%

SVM kernels are generally set to be one of the following:

\begin{itemize}
	\item Linear - $K(x, x^\prime) = x_i \cdot x_i^\prime$
	\item Polynomial - $K(x, x^\prime) = (\gamma x \cdot x^\prime + r) ^d, \quad \gamma > 0 \quad$
	\item Radial basis function (RBF) - $K(x, x^\prime) = e^{-\gamma\|x-x^\prime\|^2},  \quad \gamma > 0 \quad$
	\item Sigmoid - $K(x, x^\prime) = tanh((\gamma x \cdot x^\prime + r)$
\end{itemize}

with $\gamma = \frac{1}{2\sigma^2}$, and $r$, and $d$ are kernel parameters. 


An SVM classifer trained by Sabri et al \cite{sabri} was able to classify ripe and unripe palm oil fruits with an accuracy of $96.59\%$. They compared three methods of feature extraction (Colour histogram, colour moment, and colour correllogram) and two classification methods (SVM and Naive Bayes) in order to find the best results with SVM prevailing. Chen et al \cite{chen} performed an SVM parameter search to find the best combination whilst colour grading beef fat. They segmented the fat by use of boundary tracking, thresholding, and morphology achieving $97.4\%$ classification accuracy. Mizushima and Lu \cite{mizushima} used SVM to generate greyscale images that increased the contrast of Golden Delicious apples for segmentation purposes, before again using SVM to classify them into three grades reporting $1.42\%$ error accross all three classes. They also achieved fast processing speeds of $1.5ms$ using a $3.4 GHz$ CPU showing that SVM is capable of very fast and accurate predictions. 

However, SVM was not the predominant classifier when Alfatini et al \cite{alfatni} designed a palm oil grading application. The method consisted of using a basic grey level aura matrix (BGLAM) technique to extract features followed by an ANN classifier. After assessing ANN, KNN, and SVM classifiers, the best results were realised using the ANN with accuracy of $93\%$ and speed of $0.4s$. An ANN was also used by Bhatt and Pant \cite{bhatt} when grading apples based on size, colour, and external defects. Seven selected features were used (3 colour features, 1 size, 1 damage, 1 symmetry, and 1 weight) to train the ANN achieving $90\%$ testing accuracy. 


 


\subsubsection{Deep Learning}

Machine learning is a broad term that encapsulates Artificial Intelligence (AI) techniques adopted from many approaches, and can be defined as any  computer program that can perform tasks, after having been trained,  without having to be explicitly told to \cite{langley,shavlik,mohri}. In other words, the program can adapt and configure itself to specific conditions in order to make decisions, improve accuracy or efficiency, control systems, and predict outcomes among many other uses. The \textit{learning} is a term that refers to the process of training a program to perform a task. Given a training set $(x_i, y_i)$, $x\in X$, $y\in Y$ of inputs and outputs, the program must derive a function $f$ that satisfies $f(x_i) = y_i$ for all $i$. A very large training set will improve accuracy, however, this may result in extremely long periods of training (weeks in some cases \cite{bottou}), or over-fitting problems. Training can be sped up, however, by increasing computational power such as high-end GPU's or supercomputer systems.

Neural Networks, in AI, are a commonly implemented method for perfoming many operations and are made up of one or more layers of neurons as shown in Figure \ref{fig:neural_net}. The layers in this image are fully connected, meaning that each neuron in the layer $L_n$ takes inputs from the layer at $L_{n-1}$, and can output to the layer at $L_{n+1}$, however, neurons in the same layer may not be connected. This is the most common approach, although, sometimes it is beneficial to avoid fully connected layers. An SVM classifier can be thought to be modelled as a single layer NN, and other terms have evolved such as Artificial Neural Networks (ANN) and Multi-layer Perceptron (MLP). However, more complex topologies can be constructed with the addition of convolutional steps between layers, known as a Convolutional Neural Network (CNN) and the term Deep Learning or Deep Neural Networks (DNN) where the depth is seen in the number of connected layers.  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{neural_net.jpeg}
	\caption{Neural network topology with 2 hidden layers.}
	\label{fig:neural_net}
\end{figure}

Figure \ref{fig:neural_net} shows the flow of information through the layers in a neural network as described in \cite{smith}. Each of the inputs $Xn$ is fed to the first hidden layer with a relevant weights $W_{i,j,k}$ and biases $B_{i,j,k}$ associated for each of the neurons. Each neuron sums the weighted inputs and the value interpreted by the activation function is sent to the next layer, with yet another set of weights and biases, for each neuron. Activation functions are usually in the form of linear $y=x$, sigmoid $y=\frac{1}{1+e^{-x}}$, or hyperbolic tangent $y=\frac{1-e^{-2x}}{1+e^{-2x}}$.


For an activation function $f$, the $i$th neuron $a$ is given by:
\begin{equation}
a_i = f\bigg((w_i^0 x_i^0 + b_i) + (w_i^1 x_i^1 + b_i) + (w_i^2 x_i^2 + b_i) + ... + (w_i^n x_i^n + b_i)\bigg)
\end{equation}

or by summation: 
\begin{equation}
a_i = f\bigg\{b + \sum_{i=0}^{n} w_i x_i \bigg\}	
\end{equation}

After computing these neuron activations, the Back-Propagation (BP) process is used in order to update the weights and biases. This is done by firstly assessing the cost of the output of the network versus the training sample value ($y_i$). The cost $C_{w,b}$, where $w$ is all weights within the network, and $b$ all biases, is given by:
\begin{equation}
C_{w,b} = \frac{1}{2n} \sum_{i=0}^{n} \|y_i - a\|^2	
\end{equation}
This cost function is minimised (training process) by using methods such as gradient decent to arrive at the optimal solution, where the small change in cost $C$ takes the form:
\begin{equation}
\Delta C \approx \frac{\partial C}{\partial w} \Delta w + \frac{\partial C}{\partial b} \Delta b	
\end{equation}

These parameters are calculated and updated at each pass (epoch) and therefore may be set to some value initially, but may only be changed by the computations thereafter.

Machine learning programs have been adapted in areas such as text categorization \cite{sebastiani}, handwriting recognition \cite{bottou,bahlmann}, facial recognition \cite{bartlett,bartlett2,mohammed}, medical diagnosis \cite{shipp,ye,dreiseitl}, gesture recognition \cite{lustrek,rautaray}, image classification and computer vision \cite{chapelle,ciresan,krizhevsky}, search engines and chat bots \cite{boyan,graepel,jia}, marketing/data mining \cite{cui,bose}, and an endless amount of other applications. This technology has seen rapid popularity and growth due to the flexibility and adaptiveness of such algorithms, which allows it to be used by many companies and individuals to improve systems performance. 

In recent years, self-driving cars, trucks, taxis, mining equipment, and UAVs are using ML to process images, and interpret sensor data, to recognise elements and obstacles required to operate safely. This autonomous revolution is driving machine learning research and development in great popularity \cite{lecun}. Starting with DARPA, large companies such as Google, IBM, and Microsoft have invited hobbyists, programmers, and computer scientists - anyone who has the ability - to contribute to their AI systems, processes, and libraries with solutions to problem domains they set \cite{tensorflow,cortana,udacity,xprize}.


A comparison of six deep learning architectures performed by Koirala et al \cite{koirala} in order to grade mangoes showed that all networks can perform with accuracy $>90\%$. The architectures evaluated were Faster R-CNN(VGG) (original version, and input dimension altered), R-CNN(ZF) (original version, and input dimension altered), SSD (original version, and input dimension altered), YOLOv3 (altered), YOLOv2 (original version, and input dimension altered), YOLOv2(tiny) (original version, and input dimension altered), and a specialised network based on YOLO MangoYOLO. The MangoYOLO network has slightly higher F1 score of $96.8\%$ with the inference speed at $<70ms$ for all networks using an NVIDIA GTX 1070 Ti GPU and $32GB$ of RAM.

Ma et al \cite{ma} developed a CNN to perform disease recognition among cucumber leaves including anthracnose, downy mildew, powdery mildew, and target leaf spots. The disease is more easily and less laboriously found on the leaves of the cucumber plant as opposed to the fruit itself. The team used a DCNN to perform the detection with results of $93.4\%$ which outperformed alternative algorithms such as SVM, Random Forrest, and AlexNet.

   
A Faster R-CNN network was used by Sa et al \cite{sa} to demonstrate it's adaptability of perform fruit detection in an orchard. Initially targeting sweet pepper, they fine-tuned a pre-trained network to generate boundng boxes around the fruit instances in each image achieving $F1=0.828$. Extending the work to other fruit orchard datasets with rock melon, strawberry, apple, avocado, mango and orange also delivering good results with scores of $F1=0.848$, $F1=0.948$, $F1=0.938$, $F1=0.932$, $F1=0.942$, and $F1=0.915$, respectively. 




\subsection{Control Systems}
\label{sec:control_sys}

Vision systems have been utilised on many production lines since the early 1980's \cite{kruger}. Computers represent a medium to perform a consistent, verifiable approach to monitoring, recording, and quality control of produced items that may be susceptable to damage, contamination, manufacturing errors, poor quality, out-of-specification features, or defects and require separation from good quality produce. Depending on the goods being monitored, there may be an option to rework, correct, repack, or remake the defected instances, therefore, the control systems will differ greatly between production lines.   

The fruit grading system by Kondo \cite{kondo} used two robot arms with 3 DOF performing a series of tasks to determine colour, size, shape, and defect. The robots alternate using suction cups to pick up 12 fruit at once in order to acquire images from the above, below, and 4 sides before pushing them into the corresponding lines. 12 camera systems in total are used, with others guiding the manipulators. This, and the project by Michalosa et al \cite{michalosa} combining mobile robots with tooled manipulators serves as examples of precision, complex, robotic control systems. However, as agricultural production line items generally are not required to be assebled or involve precise movements, control systems are relatively easy to implement, without the need for complexities such as several degrees of freedom or specialised tooling to perform the task of quality control.  

Blasco et al \cite{blasco} performed grading of pomegranite arils into five categories - White, Pink, Red, Brown, and Unwanted using several pnuematic air nozzles to eject the arils into their appropriate outlets. The system can grade a maximum of $75kg/h$ using six narrow conveyors and is highly dependant of the feeding unit to uniformly distribute the arils on the conveyor belts. As the air nozzles have a conical burst of air, incorrect rejections can occur if they are too closely distributed, or if external factors cuase the arils to roll between the vision system and control. Pneumatic air ejectors helped Nouri-Ahmadabadi et al \cite{nouri-ahmadabadi} in order to grade pistachio kernels from unwanted shells. The system also required a good separation of individual items on the conveyor in order to correctly eject the affected instances. The sorter requyired an $8mm$ separation and could process $22.74kg/h$ with an accuracy of $94.33\%$. 

A date grading system by Al Ohali \cite{ohali} used electro-mechanical sorting strips placed at the end of the conveyors which guide the fruit into different grades. This type of control must have even larger spacing due to the mechanical movement followed by a return. Systems developed such as Sa'ad et al \cite{saad} mango shape and weight grading project used push/pull actuators to remove defect items and is akin to conveyor process control (i.e. conveyor separation, interleaving conveyors, conveyor gates, etc). Makky and Soni \cite{makky} developed a grading machine for oil palm fresh fruit bunches using stepwise discrimination in order to classify into accept and reject classes with an average success rate of $93.53\%$. Their design (and in-field implementation) employed a gate driven by a limit switch to separate the good fruit from the rejected.




\subsection{Summary}


A review into applications of machine vision in produce grading showed methods vary substantially depending on application and problem. Geometrical size estimation, colour, and texture analysis are common approaches due to the inherant nature of fruits and vegetables physical appearance in correlation to ripeness or maturity. However, more complicated methods have been used albeit in a more controlled environment such as laboratory settings. Vision systems provide consistent and reproducable results with the advanteges in applications where rapid production speeds, invisible spectrums, lighting concerns, and seasonal variations are factors.

Size, shape, and volume can be estimated for produce items with the provision that the images are well-segmented individual, or group of items, and that the camera is fixed relative to the subject, and calibrated respectively to obtain a $pixel/mm$ measurement. This callibration can then be used to calculate length, width and shape from a single perspective, and with the addition of a camera orthoganal to the first, volumetric and 3D reconstruction can be achieved. RGB, CIE-Lab, or HSV colour spaces can provide all the relevant information required to differentiate or grade fruit and vegetables into catgories of ripeness or maturity levels in with high accuracy, especially compared to the subjectivity of human quality checkers. Many fruits, for example, begin green and progress to yellow, orange, or red colours which can be exploited using CIE-Lab's $a*$ and $b*$ channels. FIS have been used successfully in grading of fruit and vegetables by implementing fuzzification methods such as PCA or binning features such as colour space, measurement, and texture reducing the classification to a simple decision tree problem.  

Selection of features is an important step in producing an accurate classification or detection model. Some results show that only a few hand-selected features can be highly accurate given the features are of significance to the deterministic characteristics of the subject. However, research also indicates that extracting and applying dozens of features may not produce a model that performs as well as others, indicating a disconnection between algorithmic and human discrimination.

Physical properties can be analysed in a non-destuctive manner by way of spectroscopy - a process of measuring the internal reflectance of light through an object - using varying wavelengths in order to evaluate the internal structure, or flourescence - measuring the exitation of an object in one wavelength, generated by a light source in another. NIR and SWIR wavelength analysis can reveal information regarding sugars, acidity, moisture, and firmness with a high level of accuracy, and thermographic and X-ray imaging has also played a part in the review. This type of analysis in the ranges of $1\mu m-15\mu m$ (SWIR/MWIR/LWIR) and $0.01nm-10nm$ (X-ray) generally take longer to process at the time of testing due to the sensor requirements with a resolution-time-cost balance used in most cases depending on application, making it diffuclt to employ these systems on high-speed production lines.  

The NIR region ($750nm-1000nm$), however, can be used to effectively identify defects and characteristics in objects which would be otherwise unseen by human eyes using cheap, fast, and readily available CCD and CMOS sensors. These properties can help distinguish varieties, classify grades, and detect defects with the ability to penetrate into sub-surface of skinned fruit and vegetables. Bruise detection is a common use for NIR imaging given the cameras ability to detect reflectance in the water absorbtion bands of the spectrum,highlighting the affected areas. This is advantageous in applications where the skin is darker than the flesh such as kiwi, potatoes, pear and apples.

Strawberries are very suceptible to damage, therefore, handling and transport becomes of concern particularly if the desire is to mechanically sort them in to categories or remove defective instances. Picking in the field and packing the berries into containers generates many opportunities to bruise or injure the delicate flesh before the quality inspection process begins. However, some progress has been made in the way of robot picking that can precisely pick and place berries into the containers using machine vision to operate the picking mechanism and perform the quality or ripeness gradation, simulteneously. Using a robot to pick a strawberry could see rates of $>10s/berry$ with current technology and is therefore mostly experimental in nature given that the volume of strawberries on a typical farm or packing room. Each method to process the strawberries in this review have been singulated either by way of laboratory conditions, robotised harvesting, or human placement on a conveyor before the vision system assesses the quality. However, each of these options either costs more in terms of time, or labour, due to the delicate placement requiremed in order to yield unspoiled fruit. The calyx removal system from \cite{lin} used a system of roller rods for separation of the strawberries into singular specimens before removing the calyx with a waterjet. As the metal rollers apply friction to the berries in order to transport them, they are likely to have bruising and other skin damage after processing, which means the produce is no longer acceptable by retail markets, as was the purpose of their project. 

Conversely, the apples, figs, and citrus fruits reviewed have robust skin to protect them from being constantly moved, rolled, placed, and pushed during the grading and control processes. Speeds of $54,000/h$ for apples, $90kg/h$ for figs, $72,000/h$ for dates, and $5/s$ for citrus fruit have been noted proving that machine vision is currently powerful enough to process this type of volume if the automated handling is efficient and appropriate.

Machine Learning and Deep Learning play a vital role in produce grading, along with more naive implementations, due to their ability to discriminate between classes well. Algorithms such as LDA, QDA, SVM, ANN, BPNN may take hours to train, but can perform accurately and quickly during inference. SVM and shallow networks (such as single layer ANN or BPNN) were predominant methods to predict based on colour, size, and texture features extracted by hand, whereas, CNN and deeper networks are used where the image itself is used as input in order to predict masks or bounding boxes for identification and classification purposes.

\quad
\quad


The main body of this thesis is set out with the following four chapters: 


\begin{itemize}
	\item Chapter \ref{sec:I} - Prototype and Design
	\item Chapter \ref{sec:II} - Production Integration 
	\item Chapter \ref{sec:III} - Redesign and Upgrade
	\item Chapter \ref{sec:IV} - Linux and Deep Learning 
\end{itemize}


 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Prototype and Design}
\label{sec:I}

The general design of the system is based on the requirements detailed in Chapter \ref{sec:requirements}. Added to this are some unlerlying assumptions and design requests from Magnificent, given thier previous project experience which has provided the company with knowledge and tools to assist the development of the project. Past projects include a field robot designed to traverse the rows of strawberries planted and pick ripened fruit. This project provided experience in $12V-24V$ circuits and power design, high power LED illumination and camera positioning and registration. A second project operated a picking robot in a vertical greenhouse, where the robot was fixed to single dimension movement on rails, with the strawberry troughs rotated to bring the fruit to a picking range. This project dealt with more accurate grading, polarization of the LED light, and camera registration and sychronisation. Both of these projects made use of software frameworks such as Microsoft's Visual Studio\textregistered for application design, GUI, debugging, and deployment. A liscence for MVTec Software GmbH's HALCON\texttrademark had previously been purchased for the aformentioned projects. Therefore, the use of these frameworks was requested by Magnificent due to the successful implementation in past projects, availability, and ease of use.

Visual Studio\textregistered is an Integrated Development Environment (IDE) designed for applications based in $C\#$ or $C++$ programming languages predominantly. It provides project configuration tools, automatic library attatchment, a GUI designer and linker tool which can increase the speed of development and facillitate changes and updates in an agile manner. Although it is free to use for development under the Community edition, commercial distribution would be subject to enterprise licence. 

HALCON\texttrademark uses it's own programming language and IDE to create applications, and has the power to speed up development by implementing a vast libary of common image processing functions. The IDE contains serveral visualisation windows such as code editor, current image (overlayed with current operational markers), and a window displaying all the objects created in the program including regions, images, colour channel operation steps, and contours as wel as a list of conrol varibles (integers, arrays, floats, etc) and their values if set. As the variables are cached in a database during development, the user can interactively adjust parameters of functions and see the results by simply re-running that line of code, with the resulting changes visualised instantly. The HALCON\texttrademark application code can be converted directly to $C\#$ or $C++$ code in order for integration into comercial applications such as Visual Studio\textregistered. As Magnificent had a pre-existing liscence for this software, the efficiency increases, and compatibility of both software packages, any future development was to undertaken utilising the existing framework porfolio.


\begin{figure}[h]
	\centering
	\includegraphics[width=.6\linewidth]{strawberry_glare.jpg}
	\caption{Strawberries with dominant specular reflections reducing the ability to analyse pixels}
	\label{fig:strawberry_glare}
\end{figure}%


Polarizer material had been advantageous in pervious projects given the various lighting conditions, both outdoors and within a greenhouse by way of reducing glare. However, specular reflections are an inherant property of strawberries due to their pitted exterior, effectively making small crater-shaped edges that can cause specularities from any angle. Figure \ref{fig:strawberry_glare} shows a number of white areas around the seeds caused by direct reflection of the light source \cite{gurney}. As the craters are round, and the berry's surface is curved in 2D, the specular reflections are unavoidable.

Polarization is the process of converting electromagnetic waves of multiple polarities into a single direction polarity. Usually used for visible light or lasers, polarizers can reduce glare (for example, polarized sunglasses) or help in the amplification and directionality of lasers. Figure \ref{fig:polarization} illustrates the polarization of an EM wave, indicating that no matter the intensity of any direction, only a single polarity emerges  \cite{physicsopenlab}. Using a polarizer is an effective tecnique to reduce glare from a flat surface (polarized sunglasses are assumed to be worn horizontally, using vertical polarization in the lenses, blocks glare from horizonal surfaces such as water and concrete), but will be mostly ineffective for the curved surface of the strawberries. However, using cross-polarization (two polarisers orthogonal in direction) in order to reduce these reflections is considered, even though this will limit the amount of illumination reaching the subject significantly.

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\linewidth]{polarization.png}
	\caption{Electromagnetic polarisation. A circular or randomly polarised source becomes a single polarity after passing through the material.}
	\label{fig:polarization}
\end{figure}%

As this project would take advantage of the NIR spectrum with the intention of capturing bruising or other invisible defects, the light sources must be capable of generating the required illumination. Previous projects used LED sources as they are more efficient and can be operated using $12V$ power, but are generally limited to small bands of radiation. Some LED manufacturers claim to generate a high Colour Rendering Index (CRI), which is a measure of the comparrison to natural light. However these LED's generally do not output wavelengths greater than $750nm$ as there is no need in most cases which is much of the reason they are more efficient in terms of power. This means that LED's may not be useful and a more appropriate light source such as halogen or tungsten must be used.

It is a requirement that the punnets be imaged from top and bottom, therefore requiring an obstruction-free conveyor system to transport punnets through the acquisition section. This can accomplished by using the lip (around the top of the container) as a handle point where thin v-belts can suspend the punnets giving little visible impac to the scene and allowing full view of both the top and bottom, simulteneously.

A pneumatic ejection system will remove the defective containers from the production line. Research in Chapter \ref{sec:control_sys} indicates this is a common practice in agricultural vision systems and is facillitated by the air system provided in the requrements list. As the strawberries are already packet when entering the vision system, the entire punnet will be ejected, so that it may be repacked and re-analysed by the system.



\subsection{Project Challenges}
\label{sec:challenges}

\subsubsection{Punnet Imaging}

Shiv Ram Dubey and Anand Singh Jalal \cite{shiv} investigated fruits and vegetables as well as some fruit diseases using a multi-class SVM classfiers as a solution. Qiang LÜ and Mingjie TANG \cite{lu} devised a method for identifying Kiwi fruit, which had hidden bruises under the skin, by using hyper-spectral imaging and a parallelepiped classification approach. These experiments, as well as other work \cite{elmasry2,chiu}, have used a static imaging system. That is, the subject was stationary and usually had a clean, flat, un-obscured background that makes the process of segmentation quite trivial. ElMasry et al \cite{elmasry2} acquired images of single strawberries on a plain white background in order to perform their testing and experiments using multi-spectral cameras to detect quality attributes. 

However, the system described in this report requires images to be taken of full punnets as they move down the production line. The line speed of production equates to $2 punnets/s$ and leaves little room for camera setting adjustments such as shutter speed and gain. Additionally, the strawberries will be located inside a plastic punnet and may result in poor images due to the following:  

\begin{itemize}
	\item Punnet wall reflections - light may reflect from the strawberries and project a mirror-like reflection on the inside punnet wall. These reflections could lead to false positive rejects, as they will be transformed in colour, shape, and texture.
	\item Multiple occlusions - strawberries are tumble packed and are usually more than one layer, therefore, the fruit located on the top most layer will almost certainly occlude the berries on the bottom of the punnet. 
	\item Shadows - when the fruit is tumble packed, it can form many shadowy areas, particularly on the lower layer. The berries on top will sometimes block light from other berries, casting shadows which may inhibit assessment.
	\item Poor berry orientation - tumble packing means that the berries inside the punnet could be in any orientation. As the image will be taken from above, it could be hard to tell whether a berry is lying on its side or end. The strawberry calyx could also be clearly visible and obscuring the view of that, and potentially, other berries. 
\end{itemize}

These added problems will detriment the image processing algorithms. Morphology is challenging due to the occlusions and shadows, and colour will be difficult to assess, as the shadowy lower fruit will be darker than fruit in full light. Measurement of berries will not be very accurate as the occlusios, shadows, and orientation will dictate the accuracy of this process.



\subsubsection{Overheating}

Lighting for the cameras must have enough power to illuminate the subjects with a fast shutter speed due to the fast-paced throughput. Although high powered lighting is required, this can create other problems such as overheating, causing damage to other components, and a potential safety hazard. Focusing the lighting directly onto the strawberries may cause specularities, diminishing the information collected by the cameras and, therefore, must be diffused in one of many ways. The lighting must also emit wavelengths of importance, such as visible and IR so that these wavelengths can be viewed through the cameras. 

\subsubsection{Asyncronous Aquisition of Images}

As mentioned earlier, there will be four images acquired for each punnet that passes through the enclosure, meaning that the application in control of the cameras must be able to collate four images, taken at different times. The application will then save all the images acquired in a defined storage device in order to preserve production quality traceability.   

\subsubsection{Processing Time}

Since the introduction of the heat-seal machine, the production line has been sped up to almost the maximum capable ($120 punnets/min$), in order to increase efficiencies. This equates to $2 punnets/sec$ or $500ms/ punnet$ which must allow detection, capture, processing, and decision making. Therefore, the processing time may be as little as $300-400ms$, which detriments the ability to use complex or inefficient algorithms. State-of-the-art classifiers generally take a fair amount of time to perform evaluations with simple classifiers taking a few seconds in some examples. This will require some careful consideration if each punnet is to be assessed.


\subsubsection{Orientation of Punnets}

The strawberry punnet heat-seal machine has been designed to be as efficient as possible, which means that for maximun efficiency, more punnets should be sealed at the same time. As the strawberry punnets are rectangular, this means that they must be long edge leading into the sealer in order to be able to seal five punnets at a time rather than four. This increases the rate of the punnet sealing from $100/min$ to $120/min$, giving accelerated throughput gains, but raises problems elsewhere in the production line. 

As the vision system feeds in-line into the heat-sealer, the vision system must also process the punnets with long edge leading. Since the punnets are transported through the enclosure by suspension on thin v-belts using only the lip of the plastic punnet, it will simply fall into the bottom of the enclosure if the punnet was wrongly oriented. The packing staff also need to correctly orient the punnets on the packing line so that they enter the vision enclosure correctly. However, this may be difficult to keep consistent as the staff turnover is quite high, and each packer will pack as fast as possible due to the fact that they are renumerated respectively to the amount of punnets packed.

Therefore a fail-safe system must be devised in order to either ensure that the punnets are correctly oriented before they reach the enclosure, or there is a sensor/detection unit which will alert operators if the situation arises.


\subsubsection{Ejector Synchronization}

The pneumatic ejector system is attached to the production line outside the enclosure, and therefore requires it's own punnet sensor in order to eject a punnet as it moves past the pnuematic manifold (blower head). Added to this, the punnet sensor inside the enclosure must be perfectly synchronised to the punnet sensor at the ejector so the correctly identified reject punnet is ejected and not an ajacent punnet. The ejector accuracy is proof in the system for operators and must be able to correctly eject the failed punnets for inspection, otherwise confidence in the system will reduce.


\subsubsection{Conveyor Motion}

The punnet heat-seal machine is designed to seal five punnets at a time and is therfore not in constant motion. It has a sensor at the infeed to the sealer which tells the machine when the required amount of punnets have passed into the sealing apparatus. This sensor may also stop the infeed line (approximately $1s-2s$), to prevent more punnets entering. The conveyors into and out of the enclosure are controlled by this mechanism as well. This is to ensure that if there is a problem in the heat-seal machine, and operators use the e-stop or safety switches monitoring the access doors to stop the production line, the enclosure conveyors stop as well.

However, this means that the coveyors could stop at any time during the sensing, acquisition, and ejection phases of the vision system. Therefore, time-determinant processes (such as using a finite time delay between camera sensors, or camera and ejector sensors) may not be appropriate. 


\subsubsection{Flickering}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{flicker.png}
	\caption{Six images taken using identical properties, and threshold, at varying times within one second using AC-powered light source.}
	\label{fig:flicker}
\end{figure}

As the strawberry punnets pass along the production line at a rapid pace, the shutter speed of any cameras used will need to be able to capture without blurring, but with enough light to illuminate the berries to the desired levels. The fast shutter speed requires high-powered lighting in order to allow the sensor to accumulate enough photons in such a short amount of time.

High-powered lighting is generally in the form of 110V/220V AC powered lamps which can generate over $1000W$, however, the alternating current poses the problem of flickering. This occurs when the shutter speed of the camera is $<<1s$ and the sensor can be exposed to maximum light, when the light source current reaches maximum, and theoretically, no light at all when crossing the 0V point in the alternating cycle. Some examples of threshold differences are shown in Figure \ref{fig:flicker} where only time was varied.



\subsubsection{Specular Reflections}

Specular reflections occur when the viewer is in the path of directly reflected light from a source. In other words, the light source can be seen on the surface of an object, and is usually a white looking glare which can add unecessary noise to images resulting in loss of information as described in Figure \ref{fig:strawberry_glare}.

This is due to Snell's law which states,
\begin{equation}
\eta_{1} sin\theta_{1} = \eta_{2} sin\theta_{2}
\end{equation}

When the mediums $\eta_{1}$ and  $\eta_{2}$ are the same or equal it can be seen that,
\begin{equation}
\theta_{i} = \theta_{r}
\end{equation}

where $r$ the reflected wave, is at the same angle as $i$ the incident wave from the normal of the surface. Given that the strawberry flesh's refractive index is great than air, the incident and reflected angles will not be the same. However, as the strawberry is curved surface elements of different sizes, many specularities occur. 

There are a few methods to combat the specular reflections, such as polarizing the light, changing the angle of the source, or diffuse lighting. In the case of this project, all three are implemented in order to reduce the specularities to a minimum. Polarization of a light source (using a polaroid sheet) will reduce the overall light intensity by a factor of $\frac{1}{2}$ due to Malus' law given by
\begin{equation}
	I = I_{0} cos^{2}\theta_{i}
\end{equation}

with $I_{0}$ being the initial intensity and $\theta_{i}$ is the angle of the polarizer axis from the angle of initial polarization. If a light source is unpolarized, it is thought to have a combination of all polarization directions and so $cos^{2}\theta$ is averaged to $\frac{1}{2}$. Although, in practice, this figure is less than this due to imperfections in polarizing material, more light will be required in order to achieve the same luminosity on the subject \cite{rox, sommer} whilst reducing/removing the intense reflections. Specularities appear the same colour as the light source, which in most cases is white, effectively 'blinding' the cameras from true information behind the glare.  






\subsection{Prototype Construction}

The enclosure was firstly constructed offline as a bench prototype (Fig. \ref{fig:bench_construct}), in order to test lighting arrangements, polarizers, and cameras. This prototype was then used as the framework when moved from bench to production. The frame, constructed from aluminium modular T-slot ($20mmx20mm$) components, supports stainless steel sheets that are molded to be curved on top and bottom in order to diffuse and direct light and reduce flat surface reflections whilst creating a more evenly distributed background and intensity. 


\begin{wrapfigure}{r}{0.5\textwidth}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{images/bench_construct.jpg}
	\end{center}
	\caption{The enclosed prototype during construction and testing.}
	\label{fig:bench_construct}
\end{wrapfigure}   

Front and back panels were added with hinges to allow access for staging and adjustemnts but block the outside light interference. Halogen work lamps were used to supply lighting as they were capable of emitting the full spectrum required for both the RGB and IR imaging systems. They are readily available, with power up to $500W$ per lamp and a footprint small enough to fit eight inside the enclosure. Several lighting and polarizing configurations were tested (using a thin film type polaroid) for optimization (Figures \ref{fig:bench_hal_film} and \ref{fig:bench_hal_film2}), before the AC flickering problem was realised. This test was successful in proving that the polarizer reduced specularities, and that powerful lighting was required, however, it highlighted the inherant problem with AC lighting and fast shutter speeds. It was also noted during these preliminary tests that the polarizer film would be less effective when bent or warped.  

Converting the system to $12V$ power was required in order to provide a more constant current and, therefore, lighting intensity. An LED's spectral response depends on manufacturer and can range from very poor CRI with 3 distinct bands of R, G, and B (similar to a typical flourescent lamp), up to $95\%$ CRI that not only emit some IR light, but also render blue colours better than halogen \cite{lumicrest}. However, the IR radiation will cut off around $780nm$ (enough to cover the visible spectrum completely) and therefore will not be suitable for the IR imaging system. $12V$ halogen lamps are commonly used as downlights in residential and industrial applications as they ar emore efficient, reliable, and the beam is more directional compared to their $240V$ counterpart. Therefore, Auotomotive LED light bars replaced the halogen work lamps for the visible spectrum, whilst $12V/35W$ halogen down lights provide the IR spectrum. Figure \ref{fig:bench_led_rigid} depicts the final configuration of LEDs and polariser.


\begin{figure}[h]
	\centering
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\linewidth,angle=270,origin=c]{bench_hal_film.jpg}
		\caption{}
		\label{fig:bench_hal_film}
	\end{subfigure}%
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\linewidth,angle=270,origin=c]{bench_hal_film2.jpg}
		\caption{}
		\label{fig:bench_hal_film2}
	\end{subfigure}%
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\linewidth,angle=270,origin=c]{bench_led_rigid.jpg}
		\caption{}
		\label{fig:bench_led_rigid}
	\end{subfigure}%
	
	\caption{Left to right: (a)Halogen work lamps tested facing upwards with a thin film polaroid , (b)Halogen work lamps in a diagolally upward direction with a dark-field blocker, (c)Installation of LED light bars and rigid polarizing material.}
	\label{fig:test3}
\end{figure}


An independant power supply was needed to power the prototype, which at this stage, only included the top illumination cluster. The $12V/5A$ power supply was only able to power either the LEDs or halogens for testing purposes, and not both at once due to the current limitation, indicating that a more powerful source was required.   





\subsection{Strawberry Quality Assurance (SQA) System Design}


The Computer Aided Design (CAD) generated cross-section of the proposed SQA system in Figure \ref{fig:cross_sec} illustrates the concept of the conveyors (in-feed, out-feed, and v-belt), placement of cameras and PC, as well as the polarizer sheet. This illustration integrates the already existing frame constructed for the prototype, therefore the prototype could be retrofitted to include in-feed and out-feed conveyors, and the v-belt system.  



\begin{figure}[h]
	\centering
	\includegraphics[width=.8\linewidth]{QAS_cross_sec.jpg}
	\caption{Cross-section of the SQA system illustrating the polarizing filter (blue sheet), the v-belts, and four cameras in position above and below.}
	\label{fig:cross_sec}
\end{figure}%



The PC is placed under the out-feed conveyor (as there is less probability of impact hazard), and runs the master program which controls every peripheral including cameras, peltier devices and fans, pneumatics, operator controls, image processing, and GUI. The microprocessor is a stand-alone industrial board (Phidget\texttrademark) and controls the I/O as described in Section \ref{sec:phidget}.

As indicated in the requirements (Section \ref{sec:requirements}), the farm pack shed will provide power of either $240VAC$ - single phase, or $400VAC$ - three phase as well as a single compressed air line. However, the system is designed to use only $240VAC$ as higher power is not required in order to perform all the necessary functions including conveyor motion.



\subsubsection{Hardware}

The PC consists of an Azus\textregistered P8Z68 Deluxe Gen3 Motherboard, with a $3.4GHz$ Intel\textregistered Core\texttrademark $i7$ CPU, $8GB$ DDR4 RAM, and a $500GB$ Western Digital\textregistered hard drive. Extra filters added to the PC case prevent excess dust and dirt from prematurely aging the sensitive electronics or creating failures due to build up. USB3 Gen1 offers $5Gbps$ bandwidth per host controller, however, many USB3 ports, both on motherboard and PCIe additions, share a host controller accross up to 8 USB3 ports, reducing the individual throughput of each port. Common uses for USB3 would use only one or two ports at a time where processes are not time critical, additionally if bandwidth limit is reached, the transfer rate will be reduced. However, for the SQA application to run smoothly, a board must be used where each port has it's own dedicated host controller, such as the 4-port Advantech\texttrademark PCI Express x4, 4-Port USB 3.0 Expansion Card. 

The pneumatic system is driven by a main air compressor that supplies the strawberry packing shed with air in order to operate various machinery, including the heat seal machine. A regulator is added with the soleniod controlled by the Phidget\texttrademark microprocessor. A compressed air nozzle is added to the out-feed conveyor opposite the exit ramp in order to supply a short (approx. $0.1s$), pressurised (up to $10bar$), air burst when a defect punnet is detected. The nozzle is designed for this purpose with a horizontal array of small apertures that can exert a jet pattern of $100mm$ x $55mm$ at $300mm$ from the nozzle. Only one bar of pressure is reccomended at that distance, however, as the punnet is fairly heavy ($250g-300g$), the required force may be more.

Flir\textregistered is a leading industrial, machine vision, surveillance, defense, and thermal camera company with a broad range of cameras to choose from depending on application. The Flir\textregistered cameras used for the SQA inspection are Blackfly\textregistered USB3 $2.3 MP$, 1920 x 1200, $41FPS$, with a Sony\textregistered IMX249 sensor capable of shutter speeds in the range $19\mu s-3.9s$ making it suitable for fast-paced applications. Quantum Efficiency (QE) of the mono model is approximately $10\%$ at $1000nm$ which is not ideal, but offers more spectral range and less noise at NIR wavelenghts than other sensors. The cameras are powered via USB, which helps to reduce complexity, and include opto-isolated I/O for triggering. The comprehensive list of adjustable parameters make these cameras highly controllable or highly automatic, depending on user requirements, and can be programmed to function via software using the API packages. 

As the cameras have specific operating temperatures in the range ($0^{\circ}C-45^{\circ}C$), meaning they will be suceptible to overheating and must be monitored (via the on-board temperature sensor, accessed using the API), particularly given that farm temperatures can reach $45^{\circ}C$ during the day. Therefore fans and peltier devices are added to the cameras in order to allow cooling if required. Intake fans apply positive pressure to the entire unit in an attempt to keep dust and dirt out of the acquisition area as lens/diffuser/polariser cleaning may become a common occurance.


\paragraph{Hardware Control}
\label{sec:phidget}

Various hardware devices will need to be controlable via the main application, to enable efficient main lighting, signal lighting, peltier, pneumatics, and conveyor control, as well as inputs from some peripheral devices and sensors.



\begin{wrapfigure}{r}{0.6\textwidth}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{phidget.jpg}
	\end{center}
	\caption{Image of the Phidget\texttrademark I/O board and the interface connectors on the top and bottom.}
	\label{fig:phidget}
\end{wrapfigure} 

The Phidget\texttrademark is an industrial-style microcontroller with integrated I/O that can be interfaced with a PC and has industrial screw-clamp electrical connectors. An advantage of this microcontroller is that it can be programmed in $C\#$ programming language, therefore interfacing with the main application will be simplified. The interface board has 16 inputs with $10k\Omega$ input impedance and logic levels either $0V-5V$ or $0V-12V$, as well as 16 relay outputs which can support up to $30VDC$ at $2A$ per channel. 


Given the Lighting must be only turned on when in operational mode, and draws $>2A$ current, the power is switched through a relay bank with the Phidget operating as the controller. This saves energy and lengthens the lifetime of the lighting devices, as well as limiting heat generation.

The ejector is a controllable pneumatic system which can release a burst of air on cammand for a given length of time. The airflow is mechanically adjusted to begin with, then a logic-level $12V$ is applied to engage and disengage the pneumatic actuator. In this way the Phidget can send a signal at the appropriate time, to eject a punnet.


Figure \ref{fig:phidget} shows the screw-clamp I/O connectors on the top and bottom of the image, and the USB connection point for the PC. The Phidget has a $24MHz$ processor, 256 bytes of RAM, and up to $8kB$ of flash storage which makes the device a good option for input/output processes.
 


The system topology in Figure \ref{fig:overview} describes the control relationship of the PC, microcontroller, relays, PSU and peripheral devices. The PC and PSU are supplied with $240VAC$ power and the PC provides power for the cameras. All other devices (excluding conveyors) are powered by the PSU including the microcontroller, which signals the relay bank to switch the high powered curcuits such as LED's, halogen lamps, and pneumatics. 


\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{overview.png}
	\caption{Overview of the electrical connections and control hierarchy of the SQA system.}
	\label{fig:overview}
\end{figure} 

The relays are wired in a normally closed configuration so that the relay must be activated in order to turn on the high powered devices such as lighting. Therefore, malfunctions causing any of the components to fail will result in the system's critical devices to be powered off to prevent the likelihood of damage and waste of power.


\subsubsection{Electronics and Power}

The electronics and power supply will accompany the enclosure to the production floor given that DC power is required to avoid flickering. Furthermore, the control system microprocessor, fuses, wires and relays need to be installed in order to operate the various controls. As the vision enclosure does not allow for power supply storage or peripherals inside, an electronic component box is used to contain the abovementioned items. 

Safety is of great concern given that any packing floor operator may com into contact with the system. Therefore, the design must address critical safety hazards such as electrocution, shock, arcs or short circuits (potentially causing fire) and eye damage from intense lighting. Fuses are added to the high current wiring delivering power to the main consumption devices (and to some lower powered devices) which will break the circuit on overload. Normally closed relays are used to switch high power circuits, given they will also beak the circuit on overload or malfunction.  


All electronic requirements are as follows:



\begin{itemize}
	\item PC for image processing and control of devices and hardware
	\item Uninterruptable Power Supply (UPS) for PC continuity
	\item 2 x $240V$ conveyor motors
	\item Lighting for illumination
	\item Cameras 
	\item Punnet sensors
	\item Microcontroller for hardware operation
	\item Relay bank to switch high-power sources
	\item Peltier devices
	\item Intake/exhaust/cooling fans
	\item Various circuits/wiring/connectors/fuses
\end{itemize} 

The first three items require AC mains power ($240V$) and can therefore be addressed by using readily available extension leads and a power board. The reamaining items use DC power and need to be quantified in order to design the power suupply system.

\renewcommand{\arraystretch}{0.8}% Tighter table height
\begin{table}[h]
	\caption{Table of DC power requirements used in the SQA vision system.}
	\label{tab:DC_power_table}
	\begin{tabularx}{\linewidth}{p{6cm}ccccc}
		
		\toprule
		\textbf{Element} & \textbf{Volts ($V$)} & \textbf{Current ($A$)} & \textbf{Power ($W$)} & \textbf{No.} & \textbf{Total Power ($W$)}\\[8pt]
		\midrule
		
		Exhaust Fan & 5 & 0.1 & 0.5 & 1 & 0.5 \\[4pt]
		\midrule
		Phidget Logic Power  & 5 & 0.1 & 0.5 & 1 & 0.5 \\[4pt]
		\midrule
		Peltier & 5 & 1.5 & 7.5 & 4 & 30 \\[4pt]
		\midrule
		Peltier Fan & 12 & 0.1 & 1.2 & 4 & 4.8 \\[4pt]
		\midrule
		Phidget Supply Power & 12 & 1 & 12 & 1 & 12 \\[4pt]
		\midrule
		LED Light Bar - $72W$ & 12 & 6 & 72 & 4 & 288 \\[4pt]
		\midrule
		Halogen - $35W$ & 12 & 2.9 & 23.3 & 8 & 280 \\[4pt]
		\midrule
		LED Light Bar - $30W$ & 12 & 2.5 & 30 & 4 & 120 \\[4pt]
		\midrule
		Photoelectric Sensors & 12 & 0.1 & 1.2 & 4 & 4.8 \\[4pt]
		\midrule
		Pneumatics & 12 & 0.3 & 3.6 & 1 & 3.6 \\[4pt]
		\midrule
		Traffic Lights & 12 & 0.3 & 3.6 & 1 & 3.6 \\[4pt]
		
		\midrule\midrule
		\textbf{Total DC system power requirements} &  &  &  &  & \textbf{747.8}\\[8pt]
		\bottomrule
		
	\end{tabularx}
\end{table}
 

Table \ref{tab:DC_power_table} shows the requirements for the DC powered devices in the SQA vision system. As the total power accumulates to $747.8W$, a high powered AC/DC converter is required to be constructed or purchased in order to generate the DC current to supply all devices.

Industrial power supplies can be expensive, have long lead-times (for initial construction and replacements), and may not have internal protections (such as short-circuit, over-current, thermal) and multiple outputs. However, a high-end gaming PSU can provide these features including automatic fault detection and shut-off, multiple outputs (can be configured due to the wiring loom containing several high current wires), and greatly beneficial $5V$ and $3.3V$ outputs of lower current, which can be used to power logic circuits. 


\begin{figure}[h]
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{PSU_2.jpg}
		\caption{}
		\label{fig:PSU_2}
	\end{subfigure}%
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.8\linewidth, angle=270]{PSU_3.jpg}
		\caption{}
		\label{fig:PSU_3}
	\end{subfigure}%

	\begin{subfigure}{0.8\textwidth}
		\centering
		\includegraphics[width=.70\linewidth]{PSU_4.jpg}
		\caption{}
		\label{fig:PSU_4}
	\end{subfigure}%

	\caption{Various construction stages of PSU (a) wiring and testing performed on the bench, (b) two separate circuits (rails) with $12V$ lines in yellow and $GND$ lines in black connected to fuse boxes and copper grounding bars, repectively, (c) PSU installed inside metal box with fuse boxes and lighting wiring exiting in the top right.}
	\label{}
\end{figure}


A Cougar\texttrademark $1200W$ gaming PSU is used for the DC power requirements with 2 x $12V$ high power rails at $100A$ maximum current in total. It also supplies 4 x $5V$ and 2x $3.3V$ lines, and a low power $-12V$ line giving the advantage of supplying up to $24V$ with low current. All of these outputs can also be fused using automotive in-line fuses, protecting each DC circuit. 


The PSU is designed specifically to connect to a PC motherboard and its peripherals such as hard drives, fans, and graphics / PCI expansion cards using unique connectors which only allow certain devices to plug into each. Not only does the vision system does not require these connectors, the power distribution is concentrated into it's two $12V$ rails, rather than the default - multiple, low power lines distributed to many devices. The picture in Figure \ref{fig:PSU_2} was taken during a bench test of the PSU after connectors were removed, and power and ground lines bundled. A damage prevention safety measure employed by many PSU manufacturers requires units to have a load attached in order to prevent accidental powering whilst partially or not connected. This is addressed by adding a high power, small resistor ($1W/10\Omega$) accross the load detector circuit (white ceramic component in Figure \ref{fig:PSU_2}).  


Each $12V$ power line is merged into one of two fused ditribution boards with respective ground lines connected to one of the two brass grounding bar (Figure \ref{fig:PSU_3}). Both the distribution box and ground bar are rated for $100A$, with current being drawn as required up to the fused limit for each circuit. That is, any of the outputs can be used for high or low powered devices as long as the appropriate fuse is used, and the total current requirement for all devices does not exceed $100A$. 





\subsubsection{Software}
\label{sec:software}

Research and experimentation into lighting, polarization, and mechanical construction, was performed in parallel with familiarization and experiments using the software packages provided. HALCON\texttrademark has developed their own language (HDevelop\texttrademark) which has three main points of difference with other languages. Firstly, each variable is categorised as either a \textit{Control Variable} (int, float, tuple, array, etc), or an \textit{Iconic Variable} (image, region, contour, mask, etc) which can be inspected at any point in running, debugging or stepping through a script. Secondly, only discrete operations between control variables can use the '$:=$' assignment operator as all function arguments contain both input and output (return) variables. For example a function to count regions in an image and paint the number requested, might take the form:

\begin{lstlisting}
num = 3
img = cv2.imread('image.png')
ret_image, count = count_and_find_largest_regions(img, num)
\end{lstlisting} 

would be written in HDevelop\texttrademark as:

\begin{lstlisting}
num := 3
read_image(img, 'image.png')
count_and_find_largest_regions(count, ret_image, num, img)
\end{lstlisting} 

From which point after 'ret\_image' and 'count' will take the values assigned within the function. Each function takes the form (omitting missing parameters):

\begin{lstlisting}
function([, out_control : [, out_iconic : [, in_control : [, in_iconic]]]])
\end{lstlisting} 

where lists of iconic, control, input, and output variables can exist or not.


Lastly, HALCON\texttrademark is an interpreted language with the unique property that stores all variables in a database cache as the program runs (in development, using IDE). This means that as each variable is assigned, it is shown in either the \textit{iconic variable} window, or the \textit{control variable} window, giving the user immediate response to code changes and a visualisation of the results. This allows for increased efficiency in development of image processing techniques. The screenshot in Figure \ref{fig:halcon_ide} has the IDE split into four windows, displaying the relevant information for agile development.  


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{halcon_ide.png}
	\caption{A screenshot of the Halcon IDE, from left to right, showing the image (Graphics), method (Operator), control/iconic variables (Variable), and script (Program) windows.}
	\label{fig:halcon_ide}
\end{figure} 


There are two methods of integrating HALCON\texttrademark with other applications, given that HDevelop\texttrademark is not compatible with other programming languages. An entire script may be inserted in the filesystem, such that it may be called from the main application, after importing the appropriate libraries. This method uses a wrapper function from $C\#$ or $C++$ to call a native HDevelop\texttrademark script as an \textit{External Function Call}. The other method uses the same library to program HDevelop\texttrademark objects in $C\#$ or $C++$ native language. In other words, the operations performed are written locally and natively, using external libraries of objects and operations. HALCON\texttrademark can also convert and export code directly into $C\#$ or $C++$ in order to be used as a dynamic class which would require regular updating, changes or improvements.

Visual Studio\textregistered is a well know development platform whose native programming language is $C\#$, embedded in a rich, professional IDE where creating state-of-the-art apps is common, given the UI development tools (for GUI apps), library linking, debugging tools, git, and deployment features. However, the most important parts of the SQA application, namely the GUI and image processing are supported thus making both of these application suitble for the project.


\paragraph{User Interface}

The GUI must be simple to use but capable of displaying all the relevent information to inform operators of the status, history, and show images of the punnets being processed. Various controls and indicators are required for both operation and development/debugging such as start/stop buttons, images from all four cameras, camera temperature monitoring, number of punnets assessed, reject information, and sensitivity adjustment. Figure \ref{fig:GUI} presents a screenshot of the GUI after many development iterations and testing. Green bar indicators were added which change colour if either a defect is detected (right hand side indicators), or system status changes such as stop/start, or thread broken (left indicators).  


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{GUI.png}
	\caption{GUI for the SQA vision system prior to the fourth camera being added.}
	\label{fig:GUI}
\end{figure}
  
  
HALCON\texttrademark's integration capabilities allow the GUI to display a HWindow\texttrademark directly, giving the application the ability to show regions, lines, contours, images, or text generated by the image processing algorithms. 

As HALCON\texttrademark is designed to process images and video streams, it has been developed to include a many camera protocols such as 1394IIDC (FireWire\texttrademark), GigEVision\textregistered, USB3Vision\textregistered, $\mu$eye\texttrademark, including Microsoft\textregistered's DirectShow\textregistered for generic cameras, and support to grab images from file. For each of the protocols, multiple camera manufacturers (even some camera models) are specifically implemented within the IDE along with a camera connection wizard and code exporter. The wizard allows experimentation with the various controls in GUI style, before exporting the required code to repeat the actions in a new script. 

The application waits until the acquisition sequence is complete before processing and displaying the punnet, therefore updating the GUI at rates up to $2fps$, and might occur whilst images are being captured. This problem is solved with asynchronous threads that can perform acquisition, image processing, I/O, and display seperately yet simultaneously.


\paragraph{Asynchronous Threading}

There are four main tasks required to be performed by the application:


\begin{itemize}
	\item Image Acquisition - Four cameras with external triggers activated at intervals set by the microcontroller.
	\item Image Processing - Analysis of all four images.
	\item Input / Output - Management of signals to and from microcontroller including ejection, errors, and buttons (physical and GUI simulated).
	\item Display - The view is handled by the main thread which spawns the peripheral worker threads.
\end{itemize}


Each of these functions are dependant of each other, but may occur simultaneously. For example, punnet number $n$ may be in the process of analysis (assuming this is the most time consuming thread), whilst punnet number $n+1$ is in acquisition stage. The cameras cannot afford to wait for other processes to complete before acquisition, due to conveyor motion, and must respond immediately. Similarly, the image analysis may require the majority of the $500ms$ window in order to process four images thouroughly. Furthermore, the image processing cannot occur without complete acquisition (all four images), likewise the GUI cannot display, or IO eject, without being processed first. This means signalling semaphores must be used to send messages between threads and allow each process to know the application state at all times.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{software_flow.png}
	\caption{Flow chart of the main application with each of the five threads (including main) and thier progress reporting.}
	\label{fig:software_flow}
\end{figure} 


The lighting enclosure is designed with visible RGB on one end and halogen IR on the opposite end (in the direction of punnet flow). In order to maximise lighting at the correct spectral wavelengths, at the optimal time, the RGB and IR cameras are separated to be centred in the respective lighting environments. Taking this into consideration, a fifth thread was added to monitor the RGB and IR camera triggers seperately. Figure \ref{fig:software_flow} ilustrates the flow of each thread along with it's \textit{progress reporting} action. A $C\#$ thread can report it's progress at some time during the threadded procedure by using a protected method (semaphore/mutex) to share information outside it's scope. 

Firstly, the main thread spawns four worker threads; one for each set of cameras, one for image processing, and one to monitor I/O. The RGB thread waits for the image to begin being buffered by the cameras (triggered by the sensor) before beginning the process of image collection and confirmation. The IR thread waits for the confirmation (semaphore) that the first images have been acquired, before beginning a similar process to receive and populate the remaining images for the current punnet. Once the all images are acquired, another semaphore is used to signal the image processing thread to begin. Each of these processes returns to wait for semaphores each iteration which also greatly helps in the synchronisation of punnet images. The last thread continually interrogates each of the I/O components by way of polling, using the Phidget\texttrademark's API. The hardware and software is designed to create an interrupt event if any of the input ports change state. Therefore, methods to handle each state change (low-->high and high-->low) for each input is required. The inputs are polled once every 200ms in order to give a response from the system in a timely manner, both for safety and operator useability.

The progress reporting for the RGB thread performs camera/image error checking, and signals the IR thread, whose progress confirms image count and assigns a punnet number, before signalling the image processing thread to begin. The image processing thread finalises the results and signals the main thread to display, whilst the I/O thread reports nothing until a state is changed, at which time the main thread is notified of the change.


The RGB and IR threads are not dependant or in scope of each other, however, they nedd to synchronise in order to match the two sets of images. This was achieved by using an array of 10 \textit{punnet} objects which were constantly cycled and reused within the application (as no more than 5-6 punnets can occupy the space between the cameras and the ejector). Using this method, simple logic could be used in order to confirm the punnet count matched before populating the object. 



\paragraph{Image Processing}

The image processing component is perfomed in  HALCON\texttrademark V-11.0 which is an industry standard image processing tool developed by MVTec Software GmbH. As described earlier in this chapter, HALCON\texttrademark is designed for development efficiency and rapid deployment. Consequently, a simple colour algorithm was developed in order to extract underripe pixels in the fruit. Firstly, the RGB image is transformed into HSV colour space (HSV properties shown in Figure \ref{fig:hue_sat}), to allow detection of the berry regions, and their ripeness. The saturation channel provides very good background segmentation given it's property that saturated pixels (black or white pixels) occupy one end of the values, and unsaturated (vibrant colour) occupy the other. This means the red strawberries can be extracted after some thresholding and morphology with a very accurate boundary around each berry even with the occluded nature of the randomised packing (Figures \ref{fig:bg_example} and \ref{fig:sat_thresh}).

To calculate HSV colourspace given three channels R, G, B and $Min = min(R, G, B)$, $Max = max(R, G, B)$:
\begin{equation}
	V = Max
\end{equation}
\begin{equation}
	S = 
	\begin{cases} 
		0, & Max=Min \\   
		(Max-Min)/Max, & otherwise        
	\end{cases}
\end{equation}
\begin{equation}
H = 
\begin{cases} 
rad(60) \times ((G-B)/(Max-Min)), & R=Max \\
rad(60) \times (2 + (B-R)/(Max-Min)), & G=Max \\
rad(60) \times (4 + (R-G)/(Max-Min)), & B=Max \\   
\end{cases}
\end{equation}



The following operation is performed on each S-channel pixel $S[x,y]$ to find the region $R_{berry}$:
\begin{equation}
R_{berry} = \sum_{n=1}^{P} 128 \leq S[x,y]_n \leq 255
\end{equation}

where $P$ denotes the total number of pixels. 


\begin{figure}[ht]
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{hue_sat.png}
		\caption{}
		\label{fig:hue_sat}
	\end{subfigure}%
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{bg_example.png}
		\caption{}
		\label{fig:bg_example}
	\end{subfigure}%

	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{sat_thresh.png}
		\caption{}
		\label{fig:sat_thresh}
	\end{subfigure}%
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{hue_processed.png}
		\caption{}
		\label{fig:hue_processed}
	\end{subfigure}%

	\caption{Left to right: (a) HSV colourspace showing hue circle and saturation vector, (b) raw RGB image with some backgound lighting noise, (c) saturation channel with berries segmented after pre-processing, (d) image reduced to berry region showing four small under ripe areas detected by the algorithm.}
	\label{}
\end{figure}


This is then followed by colour analysis of the hue channel in order to quantify the amount of pixels which lie outside the range $10<H<48$ where H is the hue circle from $0^{\circ}-360^{\circ}$. If we take $D$ as degrees of the hue circle, then the equation for $\alpha$, the corresponding greyvalue is calculated as:
\begin{equation}
\alpha = round\Big\{255\times \frac{D}{360}\Big\}
\end{equation}


These values can be inserted to the equation and performing the operation on the H channnel, the region $R_{colour}$ can be extracted in as follows:
\begin{equation}
R_{colour} = \bigg\{\sum_{n=1}^{P} \alpha_1 \leq H[x,y]_n \leq \alpha_2\bigg\} \in R_{berry}
\end{equation}


where $\alpha_1$ and $\alpha_2$ are the grey level upper and lower limits to threshold. For red strawberries the inital values are set to $0^{\circ}$ and $20^{\circ}$ which equates to 0 and 14 for $\alpha_1$ and $\alpha_2$ respectively.

Pre-processing consists of reducing the image to a Region Of Interest (ROI) in the spacial location where the punnet is consistently captured. The post-processing methods use morphology, removal of regions less than certain limits (noise reduction), and area calculations to return the affected areas as indicated by the ellipses in Figure \ref{fig:hue_processed}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{A Method To Create Stable Lighting And Remove Specular Reflections for Vision Systems}
\label{sec:paper_1}

\textbf{Gilbert Eaton, Andrew Busch, Rudi Bartels, and Yongsheng Gao}

\textit{Conference paper - Digital Image Computing: Techniques and Applications (DICTA) - accepted Aug 2017}


\subsection{Abstract}

\textbf{A lighting system and method has been developed which has shown in testing to allow quality images to be obtained that are free from two particular problems, specular reflections on the subject, and light intensity variation. These problems both diminish the ability to compare objects for attributes such as colour variation, edges, contours, and many other features. The system developed eliminates specular reflection by using the cross-polarisation configuration, and reduced flickering due to fluctuations in the power supply to negligible levels by constructing a high-power DC source capable of providing sufficient 12 Volt power. These two improvements create an environment suitable for taking high-quality, noise free images at high shutter speeds for the purpose of assessing the quality of strawberries moving on a real-time production line.}


\subsection{Background}


When using image processing to analyse images or extract features, one problem faced is the inconsistency of images \cite{atkinson}.  For example, when using AC lighting and fast shutter speeds, the image intensity will vary with the intensity of the alternating current in the power supply. Although undetectable to the human eye, this type of "flickering" can give rise to a vast difference in image intensity. Another example is that of specular reflection as shown in Figure \ref{fig:specular_art}. The bright reflection appears as white "interference" as the information contained in the pixels of this region are lost. The loss of information may include colour, edge contours, region boundaries or hidden defects. This paper is aimed at describing a method of eliminating these problems in order to increase consistency in acquired images.

Image stability is particularly required in industrial applications when performing defect inspections, or quality control processes. The environment surrounding the subject must be consistent, and free from saturation and specular reflections. Achieving this will then provide more valuable information and comparability of images due to the removal of noise and inconsistency.

The proposed system ensures that when objects imaged for computer vision applications, they do not contain specular reflections or saturation, and intensity is stable for comparison purposes. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.9]{specular_art.jpg}
	\caption{Example of Specular Reflection on an art display. Note the specular reflection interference with the image causing the loss of information and colour.}
	\label{fig:specular_art}
\end{figure}


Specular reflections occur when the source of light can be seen on the surface of the object. This can be either a direct reflection from the light source onto the object and then into the viewer (eye or camera), or through multiple reflections where the light source is still visible. The direction of the specular reflection is related to Snell's Law, in that, the angle of incidence ($\theta_i$) is equal to the angle of reflection ($\theta_r$). These reflections are best seen on smooth surfaces such as glass, water, and metal whereas reflections from rough surfaces will result in a more diffuse spread.   

According to electromagnetic wave and transmission line theory, at the interface of a good conductor, the reflected field ($\bar{E_0^r}$ - Phasor form of the electric field in the z plane of incidence) at the interface of the medium is equal to the incident field ($\bar{E_0^i}$) subtract the transmitted field ($\bar{E_0^t}$) \cite{ulaby} where:

\begin{equation}
\bar{E_0^r} = \bigg(\frac{\eta_2 - \eta_1}{\eta_2 + \eta_1}\bigg)\bar{E_0^i}
\end{equation}

and for the transmitted field:

\begin{equation}
\bar{E_0^t} = \bigg(\frac{2\eta_2}{\eta_2 + \eta_1}\bigg)\bar{E_0^i}
\end{equation}


This shows that the intensity of the reflection is determined by the intensity of the source of the wave and the intrinsic impedance ($\eta$) of the material and is given by:

\begin{equation}
\eta = \frac{\omega \mu}{k} = \frac{\omega \mu}{\omega \sqrt{\mu \epsilon}}= \sqrt{\frac{\mu}{\epsilon}}
\label{eqn:eta}
\end{equation}

where the wavenumber $k = \omega \sqrt{\mu \epsilon'}$ and $\mu$ is the magnetic permeability of the material. This relates to its susceptibility to magnetic fields, and for diamagnetic and paramagnetic materials (which includes most metals and dielectrics), is considered to be the same as that of free space and $\mu = \mu_0 = 1$ \cite{ulaby}. 

The electric permittivity, however, is very small for most metals as $\epsilon = \epsilon_r \epsilon_0 = 1 \times 8.85\times10^{-12}$. Substituting this into equation \ref{eqn:eta}, maintains a large value for intrinsic impedance $\eta_2$. thus, the majority of the wave is reflected at this angle. This is comparable to a short circuit on a transmission line resulting in a reflection coefficient of -1 indicating a full reflection even though inverted. This shows that the intensity at the specular angle is not diminished very much by the surface of a smooth conducting surface.

However, one change that can occurs at the interface, is the polarization of the wave. All light sources are randomly generated in all orientations, and can therefore be said to be multi-linear, circularly, or elliptically polarised (commonly named un-polarised or non-polarised) waveform \cite{artusi,nelson}.


\subsection{Specular Reflections} 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.9]{unpolar_polar.png}
	\caption{The interface showing specular reflection of an electromagnetic wave in either the TE or TM orientation.}
	\label{fig:unpolar_polar}
\end{figure}

Figure \ref{fig:unpolar_polar} shows that when an un-polarized incident light ray reflects from a surface, the polarisation is converted to linear form, in the direction of the surface of the object \cite{nelson}.

As the total energy is maintained and the polarisation is converted to linear, the reflection is perceptively intensified in that direction. Multiple reflections can also contribute to this appearance of specular reflection hot spots on objects depending on the surfaces around them and the lighting orientation.

This is a problem for industrial areas where, for instance, on a production line sufficient lighting is required to illuminate the product for image processing purposes. Reflections can occur from the surrounding surfaces or objects, and can be subject to change if the environment surrounding the production line and cameras are rearranged, added or removed.


\subsection{Lighting Intensity Stability}

As discussed in Section \ref{sec:challenges}, alternating Current (AC) power sources have a sinusoidal pattern and have a frequency associated with them. This fluctuation is inherent in everyday power usage, such as the current in mains power supply (in most cases 110/220/240V @ 50/60Hz), and is very useful for long distance power transmission. Although this fluctuation can not be seen by the human eye, the fast shutter speeds of the vision cameras can. The difference in the images can theoretically be $100\%$, as the current passes through the zero point in the sinusoid.

The benefit to using AC lighting is that high power/high intensity lighting is easier to achieve. For example, $1000W$, $240V$/$50Hz$ AC lamps are readily available, however, a $1000W$, $12VDC$ lamp is substantially less common. This means that to achieve the same level of lighting as an AC system, a $12V$ DC system must be able to power many small LED, tungsten, or halogen lamps resulting in high current requirements.  


\subsection{Methods}

\subsubsection{Specular reflections}

Polarising material has been shown to reduce the effects of these reflections. If surface reflections of the light source are seen by the cameras, and not the direct light source, the polarising material can remove the specular reflections in one direction. This type of method has been used in experiments to try to improve the quality of images \cite{atkinson, wolff}.

Exploiting the use of a cross-polarising lens on the camera reduces the specularities even further. By controlling the polarity of the light hitting the objects' surface, and then again before the light enters the camera lens, the reflections in the image are minimised.

If the polarisation orientation is parallel throughout the the inner walls of the tunnel, then all light within the tunnel will have the same orientation of polarization. By adding a polarising filter to the camera lens at an angle orthogonal to the polarised walls, then the specular reflections are dramatically reduced \cite{anderson, kuranov}.


\subsubsection{Power Supply}


Preliminary testing gave a good indication of the lighting power requirements by testing many different lighting angles, intensities and polarization configurations. The tests showed that, in order to get adequate intensity through a cross-polariser, the power requirements in Table \ref{tab:DC_power_table} are determined to be appropriate.

After initial bench testing, the calculated lighting power was around $688W$, with power draw from peripheral devices totalling approximately $750W$ in normal operating conditions. A $750W$ power supply would meet the operational requirements but leaves no room for variation in current such as inrush, and the potential to increase the amount of light sources in the future. Another requirement of the power supply is that it support $5V$ for peltier, fans, and micro-controller (Phidget), otherwise an additional power supply, or a high-powered, voltage divider circuit will be required.

It is for these reasons that a PC Power Supply Unit (PSU) was determined to best fulfil the requirement. A PSU generally supplies $12V, 5V$, and $3.3V$ as well as some models providing a $-12V$ low current line in order to generate $24V$ potential. These PSUs are designed for longevity, robustness and high power PC applications and hardware used to render video, or facilitate graphics-intensive, high resolution games. The Cougar $1200W$ PSU is high quality and cost effective means to generate $100A @ 12V$ with the added attractive fault protection features. The relevant specifications are shown in Table \ref{tab:psu_spec}.


The PSU also has the following protections as part of the manufacturing specifications:

\begin{itemize}
	\item Over Voltage Protection - If the voltages increase above a certain tolerance value on the single lines, the PSU automatically switches off.
	\item Under Voltage Protection - If the voltages fall below a certain tolerance value on the single lines, the PSU automatically switches off.
	\item Over Power Protection - If the system is oversized and requires more power from the PSU than it can deliver, this protection function is activated.
	\item Over Current Protection - If the load on a single line is higher than indicated, the PSU automatically switches off.
	\item Short Circuit Protection - In the case of a short-circuit this feature prevents damage to the core components of the PSU and its system components.
\end{itemize}

\quad
\begin{table}[h]
	\centering
	\caption{Cougar $1200W$ Power Supply Unit (PSU) specifications}
	\label{tab:psu_spec}
	\begin{tabularx}{0.6\linewidth}{X*{4}{c}}	
		\toprule 
		Specification & Min & Typ & Max & Units\\ 
		\midrule
		DC Output & - & - & 1200 & $W$ \\[6pt]
		Operating Temp. & 0 & 25 & 45 & $^{\circ}C$ \\[6pt]
		Efficiency & 80 & 87 & 90 & $\%$ \\[6pt]
		Transient 12V turn on & 7 & 9 & 10 & $ms$ \\[6pt]	
		Ripple 12V (100\% load) & 70 & 73 & 75 & $mV$ \\[6pt]
		Ripple 5V (100\% load) & 39 & 42 & 45 & $mV$ \\[6pt]
		Ripple 3.3V (100\% load) & 40 & 42 & 44 & $mV$ \\[6pt]
		
		\bottomrule
	\end{tabularx}
\end{table}

\subsection{Results}

The $1200W$ DC power supply is designed and constructed so that the full power of the $12V$ rails are supplied by distributing each of the two channels between the top and bottom lighting circuits. The power supply also supplies power to peripheral devices such as pneumatics, LED indicators, warning lights, logic control board, sensors, peltier, and fans.

The resulting images taken using the designed enclosure have dramatically improved the image stabilisation when compared to the AC lighting system. The DC-powered lighting system resulted in very small intensity fluctuations, and was measured to be $<0.1\%$ as seen in Figure \ref{fig:image_diff_dc} and \ref{fig:image_diff_dc_2}. The average pixel area for each image was obtained by first taking a threshold of subjects in the each of the frames:
\begin{equation}
R = \sum_{i=1}^{P}t_1 \leq x \leq t_2
\end{equation}

with $P$ denoting the number of pixels in each image and $t_1$, $t_2$ the threshold limits. The average region area ($Area_{avg}$) was calculated and is given by the mean definition:
\begin{equation}
Area_{avg} = \bar{R} = \frac{1}{N}\sum_{i=1}^{N}R_i
\end{equation}

where $N$ is the number of images to average, and $R_i$ represents the number of region pixels in each image.

Data was gathered twice, once for a period of two seconds with a fast frame rate, and the other over a three hour period in order to assess the case of high speed cameras, and in the case of constant operation for extended periods of time.


\begin{center}
	\begin{figure}[h]
		\centering
		\begin{subfigure}{.45\textwidth}
			\centering
			\includegraphics[width=.95\linewidth]{image_diff_dc.png}
			\caption{}
			\label{fig:image_diff_dc}
		\end{subfigure}%
		\begin{subfigure}{.45\textwidth}
			\centering
			\includegraphics[width=.95\linewidth]{image_diff_dc_2.png}
			\caption{}
			\label{fig:image_diff_dc_2}
		\end{subfigure}%
		\caption{Intensity measurements taken using DC lighting system showing the small variations over: a) A two second period and b) A 3 hour period. All results have a difference of less than 0.1\%.}
		\label{fig:test4}
	\end{figure}
\end{center}

\begin{figure}[h]
	\centering
	\includegraphics[width=.7\linewidth]{polarizer_rot.png}
	\caption{Experimental results gathering the specular and region pixels, whilst rotating the cross-polariser on the camera lens.}
	\label{fig:graph_polar}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{berries_spec.png}
		\caption{}
		\label{fig:berries_spec}
	\end{subfigure}%
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{berries_no_spec.png}
		\caption{}
		\label{fig:berries_no_spec}
	\end{subfigure}%
	\caption{Image of strawberries (a) with specular reflections and (b) without specularities after the cross polariser was added.}
	\label{}
\end{figure}

In order to solve the specular reflection problem, the use of a cross-polarization lighting design was implemented. As discussed in Section \ref{sec:I}, polarizing material is installed vertically (polarization direction can be either orientation, although further testing may indicate a preference) to form a tunnel through the enclosure. The second polarizer is an adjustable filter fitted to the lens of the visible camera. The graph shown in Figure \ref{fig:graph_polar} shows that when the specular pixels are removed, the region pixels are increased giving a better assessment of the area of interest.



Figure \ref{fig:berries_spec} shows an image with only one polarizer in place allowing some specular reflections to be seen (the amount of specularities would be even greater if the acquisition had no polarizing elements), then by rotating the camera lens polarizer, images were taken at 16 positions from $0^{\circ}$ to $180^{\circ}$.  


When the polarizers are orthogonal ($90^{\circ}$), the result is a more flattened image that removes the specular reflections almost entirely.  Figure \ref{fig:berries_no_spec} depicts this change and it can be seen that the surface of the strawberries are much clearer and will allow better image processing due to the increase of pixels which can be analysed, and a decrease in noise. 



\subsection{Conclusion}

Image acquisition plays an important part in any machine vision application to ensure that the images acquired are free of distortion and provide the most information. Lighting is therefore an important factor in image acquisition systems, allowing objects surfaces to be illuminated with as little interference as possible.

A system has been developed to inspect strawberries and assess them for quality attributes. The first step in the development is to design and construct the acquisition system and integrate it with the production line. After the enclosure had been installed, the LED/halogen lighting, polarizers, and cameras were mounted. The DC power supply delivers the required amount of power at a constant current which ensures that the intensity fluctuations between images are insignificant. Experimental results show that the fluctuations are less than $0.1\%$ of total intensity, even when operating for many hours. Experiments were also performed that showed that the cross-polarization technique has the ability to entirely remove of the specular reflections in most cases, whilst providing more pixels that can be analysed in each frame.  

The system developed is extremely successful in achieving the desired consistent image acquisition. The system can image 2 punnets of strawberries per second on the production line and the image quality is illuminated, free from interference, and consistent between images, even with a shutter speed as fast as $1ms$. Future work includes the strategy to strobe the lighting in order to reduce power usage and heat, as well as the introduction of image processing algorithms to perform the assessment.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Production Integration}
\label{sec:II}


Implementation of the production-ready enclosure required the prototype to be fit with conveyors, lighting, power systems, diffusers and polarizers, as well as the computer systems, sensors and cameras. Once the designs were complete, bench testing of the equipment such as camera image transmission to the running application, microcontroller functions including camera triggering and lighting switching, power system and peripherals was performed, reducing delays when installation occurs on the busy production line.





\subsection{Production Floor Migration }


Initially, the conveyors were added to the frame in ordr to create the path of flow for the punnets, as shown in Figure \ref{fig:system_construct_3}. The frame of the prototype was adjusted slighlty in order to equal the height of the conveyor where it meets the v-belts, as well as centering the conveyor relative to the enclosure. All heights are consistent with other machinery and conveyors in the packing shed to allow many possible configurations in the process flow. 



\begin{figure}[h]
	\centering
	\includegraphics[width=.4\linewidth]{system_construct_3.jpg}
	\caption{The frame of the enclosure shortly aftert the conveyors had been attached. The v-belts have also been installed and are visible, mounted higher than the conveyor belt in order to suspend punnets by their lips.}
	\label{fig:system_construct_3}
\end{figure}%


A wide range of conveyors, motors, guide rails, and parts were made available (due to the farm equipment spare parts), and assisted the construction of the punnet conveyors, and their attachment to the frame of the enclosure. However, the v-belts are a customised feature using brackets to support adjustable grooved rollers designed for the shape of the belt, and a guide rail on each side to avoid sagging due to punnet weight. The point of transfer, where the v-belts meet the punnet conveyors, must be considered in order to implement a smooth transition at both the entry and exit of the enclosure. The requirement is for the punnet to enter the heat-seal machine long-edge leading (due to cost-saving design), but this is problematic for the v-belts in the case where a  punnet enters incorrectly oriented, and too narrow to be transfered onto the v-belts. Therefore, the design was to employ a $90^{\circ}$ transfer belt between the vision system and heat-seal machine, allowing the punnets to be short-edge leading as they enter the enclosure, eliminating the risk of containers falling between the v-belts. 


Two $100W/240VAC$ conveyor motors are used with slightly different gear ratios. The in-feed conveyor and v-belts are driven by the same motor, whilst the out-feed conveyor is driven slightly faster (using a smaller sprockets) in order to separate the punnets in case of ejection.


The ejector mechanism must be positioned some time after the vision inspection, given that the v-belts prevent any dislodgement, and the added complications associated with adding ejector mechanisms in the acquisition section. Therefore the ejection must occur on the out-feed conveyor, generated by a pnuematic burst of air from the side, pushing it in a direction perpendicular to the motion of flow, onto a roller-ramp which then guides the punnet into an accumulation area. Both the ejector and acquisition system activate when a punnet passes the photo-electric sensor. In the case of ejection, the microcontroller must be synchronised with the running application to determine which punnets to eject and will only activate the pneumatics when defects are detected. The acquisition system generates a sequence of imaging - through the hardwired triggering mechanism attached to the I/O port of each camera - to capture the different views and spectrums of each punnet, followed by instigating the image processing and display routines within the application. 


\subsubsection{Electronics and Power}


The main electronic devices can be seen in Figure \ref{fig:system_construct_1} where final testing took place before they were installed. Once confirmed, the PSU was mounted and reconnected to the distribution devices and ground bars. The LED's and halogen lamps wiring has a $3.5mm$ diameter core designed for high current to avoid any thermal concerns in the already heated environment. Each LED bar is installed with it's own power line directly to the distributor, however it is more efficient and simplified to wire the hallogen lamps in pairs (equalling the current for one LED bar). This standardises the current flow on any one of the (eight in total) lines to approximately $6A$, meaning that $7.5A$ automotive fuses were suitable protection, given they can withstand the inrush on startup. 


\begin{figure}[ht!]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{system_construct_1.JPG}
		\caption{}
		\label{fig:system_construct_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth, angle=270]{system_construct_2.jpg}
		\caption{}
		\label{fig:system_construct_2}
	\end{subfigure}%
	\caption{System integration stages (a) testing all lighting powered at once by the PSU prior to installation , (b) all LED's inplace as well as polarising material/diffusers, and a single camera installed before the addition of halogen lamps. Extra cladding added to both remove external lighting noise, and cover potential pinch-points.}
	\label{}
\end{figure}

Smaller LED bars ($30W$ opposed to $72W$) were required to be installed in the mid section of each side of the enclosure as main (top) LED's were upward-facing the illumination diminished significantly from the sides of the conveyor. These four devices were also paired together, taking up two outputs on the distribution device. 


Figure \ref{fig:system_construct_2} shows the main, and auxillary, LED's installed along with an RGB camera in order to perform some lighting testing. The diffusers and polarising sheets are sandwiched together and form a rectangular tunnel around the acquisition chamber, with the intense lighting outside diffusing and polarizing the light as it passes through. The external cladding (powder-coated steel sheets) covers each end of the lighting enclosure (above and below) and protrudes out from the rollers around the conveyors for safety purposes. 


\paragraph{Lighting Finalisation}


With the LED's in place and acquisition tests performed, the halogen lamps and brackets were added to the exit end of the enclosure. The $35W/12VDC$ lamps are designed as downlights for kitchens and showrooms using a lens with a $35^{\circ}$ apperture, in order to deliver a narrow floodlight beam. The have two pins to connect to the wiring or balast, which push in to a connector comprising of spring-loaded friction terminals to hold the bulbs in place. The brackets help the lamps to face downward at an angle of $45^{\circ}$ relative to the cameras using the theory that diffuse lighting, rather than direct, would be better to avoid specularities (Figure \ref{fig:halogen}). IR images will have less specular reflections due to the surface penetration properties found in IR wavelengths, however they may still occur. Both the polaroid sheets and the darkfield lighting strategy help to minimise the event of unwanted reflections.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth, angle=270]{halogen.jpg}
		\caption{}
		\label{fig:halogen}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth, angle=270]{system_construct_2.JPG}
		\caption{}
		\label{fig:lights_power}
	\end{subfigure}%

	\caption{(a) The halogen lamps in place at an angle of $45^{\circ}$, (b) Completed lighting arrangement with PSU, distributors, and fuses mounted in electrical box.}
	\label{}
\end{figure}


The final steps for the installation of the lighting systems required wiring and connection to the PSU distribution device. Figure \ref{fig:lights_power} shows the operation of each required LED bar and halogen lamp powered by the PSU, after running the appropriate power wiring to length. Cable management and labelling practices are necessary in order to give the system a clean and well-designed apperance, added to the methodical value it gives to the process of rewiring or fault-finding.



\paragraph{Electronic Control System}


The microcontroller must perform physical electronic tasks which are required for the application including pneumatics, lights, and physical buttons. Each item may differ in power requirements, ranging from $3.3V-24V$ in both low and high current capacities. Therefore, a relay bank is used to switch the high-powered circuits such as lighting, whilst the Phidget\texttrademark's outputs are robust and can accomodate medium and low-powered devices. 


A programatic looping worker thread services each I/O port periodically by performing three tasks; checking errors from microcontroller and other threads, processing interupts generated by peripheral inputs, and processing output actions when required. The I/O worker thread is spawned from the main program which loops at $200ms$ per iteration. Each cycle, the worker uses the Phidget\texttrademark API call \textit{IsAttached()} method to check if the device is attached to the running application. If it is not, the input interrupt cannot be registered and the actions will not occur, therefore, an error message is displayed and the program must re-start. Although this is not a common failure, it is required in order to prevent I/O communication problems in future given the long hours and constant usage. This worker also monitors the health of the remaining threads which perform acquisition and image processing. 

The inputs that need to be processed are generated by automatic processes (punnet sensors and heat-seal signal), however, the emergency stop button is wired to the conveyor power system, via a switched $240V$ device, in order to have the ability to stop them in an emergency. The switch has a dry-contact activation so any circuit which closes/opens will activate it. The punnet sensors use a $685nm$ beam with reflector that, when broken, can either transition low-to-high or high-to-low depending on requirements. All three sensors are powered directly by the PSU with the output signal connected to the input port of the microcontroller. 


\renewcommand{\arraystretch}{0.8}% Tighter table height
\begin{table}[h]
	\caption{Microcontroller input port assignments and peripheral devices power ratings.}
	\label{tab:micro_pinout_in}
	\begin{tabularx}{\linewidth}{Xcccc}
		
		\toprule
		\textbf{Element}  & \textbf{Port No.} & \textbf{Voltage ($V$)} & \textbf{Current ($A$)} & \textbf{Power ($W$)}\\[8pt]
		\midrule
		Emergency Stop Button 	& 0 & 12 & 0.001 & 0.012  \\[4pt]
		\midrule
		Heat Sealer 			& 2 & 24 & 0.5 & 12 \\[4pt]
		\midrule
		Eject Sensor 			& 4 & 12 & 0.1 & 1.2  \\[4pt]
		\midrule
		Punnet Sensor 1 		& 5 & 12 & 0.1 & 1.2 \\[4pt]
		\midrule
		Punnet Sensor 2 		& 6 & 12 & 0.1 & 1.2  \\[4pt]
		\midrule
		\textbf{Total} 			&   &    & \textbf{0.801} & \textbf{15.612} \\[4pt]
		\bottomrule
		
	\end{tabularx}
\end{table}


The heat-seal machine has a jogging mechanism to control the punnets entering the sealing press. This means that the infeed conveyors stop periodically when the press is full, creating a convergence point for the punnets which could result in a backlog. Therefore, a signal line is sent to the microcontroller so that the conveyor of the vision system mimics that of the heat-seal. The listed requirements and voltages/currents for the Phidget\texttrademark inputs are detailed in Table \ref{tab:micro_pinout_in}.

The high-powered devices are controlled by the Phidget\texttrademark in conjunction with a relay bank, in which the logic level input that drives the activation of the load requires $5V$ at $70mA$ current. The relays are rated at up to $250VAC/10A$ and $30VDC/10A$ restricting each relay to control either one LED bar ($6A$), or one pair of halogen lamps ($5.8A$). This would mean 10 relays would be required to perform this switching of components that was not necessarily needed to be automatically controlled. Therefore the LED bars and halogens were attached directly to the PSU so that they are powered continuously, and given that the PSU has it's own power switch, the illumination can be restricted to operational times only. The peltier and pneumatics remain as the next biggest loads and are therefore connected to the relays in order to prevent possible overload on the microcontroller, bringing the required number of relays to three. The ouputs of the microcontroller and respective power loads are listed in Table \ref{tab:micro_pinout_out}.


\renewcommand{\arraystretch}{0.8}% Tighter table height
\begin{table}[h]
	\caption{Microcontroller output port assignments and peripheral devices power ratings.}
	\label{tab:micro_pinout_out}
	\begin{tabularx}{\linewidth}{Xccccc}
		\toprule
		\textbf{Element}  & \textbf{Port No.}  & \textbf{Relay Trigger} & \textbf{Voltage ($V$)} & \textbf{Current ($A$)} & \textbf{Power ($W$)}\\[8pt]
		\midrule
		Conveyor Power Switch 	& 0 & No & 5 & 0.001 & 0.005 \\[4pt]
		\midrule
		Pneumatic Solenoid 		& 1 & Yes & 5 & 0.07 & 0.35 \\[4pt]
		\midrule
		Traffic Lights 			& 2 & No & 12 & 0.001 & 0.012 \\[4pt]
		\midrule
		Buzzer 					& 3 & No & 12 & 0.001 & 0.012 \\[4pt]
		\midrule
		Top Cooling 			& 4 & Yes & 5 & 0.07 & 0.35 \\[4pt]
		\midrule
		Bottom Cooling 			& 5 & Yes & 5 & 0.07 & 0.35 \\[4pt]
		\midrule
		\textbf{Total} 			&    &     &   & \textbf{0.213} & \textbf{1.079} \\[4pt]
		\bottomrule
		
	\end{tabularx}
\end{table}


The relays and microcontroller can be seen in Figure \ref{fig:PSU_5} protected and fixed inside a plastic electronics case in order to simplify the wiring as well as keeping the relay board in close vacinity to the microcontroller pins which activate them. As the pneumatics are powered by $12V$ and the peltier $5V$, a connection from the second distribution device, as well as the $5V$ terminal, to the normally-open terminals of the relays are made, before running together out of the box, to their respective components. The relays have three input pins ($5V$, $GND$, and Signal), and three output terminals (Normally Open (NO), Common (C), and Normally Closed (NC)) with a small LED to indicate activation. 


\begin{wrapfigure}{r}{0.7\textwidth}
	\begin{center}
		\includegraphics[width=.8\linewidth, angle=180]{PSU_5.jpg}
	\end{center}
	\caption{Power Suplly Unit with all wiring, fuses, distribution devices, $5V$ and $3.3V$ terminals as well as the microcontroller and relay box.}
	\label{fig:PSU_5}
\end{wrapfigure}


The $5V$ and $3.3V$ fuses and terminals are seen in Figure \ref{fig:PSU_5} (top left, black electrical box), and power the relays, however the signal is generated by the microcontroller. Eight $5V$ power lines from the PSU were bundled into two connections (as this is a more common voltage), and eight $3.3V$ bundled into one connection. The PSU specifications table states that the maximum current for either channel is $25A$ and they must share $160W$ load. Similarly, either of the $12V$ rails can supply up to $60A$ each but must share a total load of $1176W$, giving the power system the ability to dynamically distribute power as required, as well as the ability to reconfigure or add devices easily.

Wiring diagrams provided in Appendix \ref{app:wiring} detailing the complete wiring configurations accross their different versions.



\subsubsection{Cross-polarization of Cameras}


The polariod sheets have a direction of polarisation, which usually runs parallel to one of the edges of the sheet, that is, either horizontally or vertically polarised. The polarity must be continuous on all sides, above and below the acquisition chamber in order to be effective when the cross-polariser is applied. The cross-polarising element used is a camera lens filter polariser which can be rotated $360^{\circ}$. This means that the cameras can be mounted in any orientation, and still achieve the desired result by adjusting the filter (Figure \ref{fig:nd_filters}).


As described in Chapter \ref{sec:I}, the polarising material transforms the incoming light from multi-polar to single polar form, which reduces the amount of specular reflections on any object therafter. However, after applying another polariser orthoganal to the first, no specularities remain visible regardless of the multiple curvatures in the surface of the strawberries, as illustrated in Figure \ref{fig:berries_no_spec}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{cross-polariser.png}
	\caption{Example of the polarising filter used on the camera lens which can be rotated to achieve cross-polarisation regardless of camera orientation.}
	\label{fig:nd_filters}
\end{figure}

The cameras are positioned in the centre of the respective lighting clusters in terms of spectral range in order to provide the best lighting response for each image. Therefore the RGB and IR are seperated by a distance approximately $400mm$, complicating the asynchronous aquisition process.







\subsection{Final Configuration}


Testing the $90^{\circ}$ coveyor arrangement resulted in a change of conveyor configuration. If packed at production rates, each punnet collides with the next during the convyor transition creating a randomly varying angle of orientation. It is imperative that punnets enter the heat-seal long edge leading (as per the requirements), as failure means stopping the production line in order to retreive the incorrectly oriented container.

Therefore the v-belts and rail spacing required adjustment to the length of the punnet, rather than the width, creating the problem of miss-transfers and falling containers. This was addressed by adding a punnet-catching platform angled downwards from the entry transition point. Figures \ref{fig:inside_enclosure_1} and \ref{fig:inside_enclosure_2} show the punnet catcher and an in-line view of the enclosure.


\begin{figure}[ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth, angle=270]{inside_enclosure_1.jpg}
		\caption{}
		\label{fig:inside_enclosure_1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth, angle=270]{inside_enclosure_2.jpg}
		\caption{}
		\label{fig:inside_enclosure_2}
	\end{subfigure}%
	
	\caption{(a) Punnet catcher installed in order to stop falling punnets due to mis-transfer, (b) view in opposite direction of flow showing camera separation, punnet catcher, and v-belts.}
	\label{}
\end{figure}

A mechanical switch was added to the underside of the catcher so that when a punnet falls, a signal can be sent to the microprocessor, which then stops the production line and applies the buzzer sound to alert an operator. This event is far less likely than the alternative in terms of interrupting the production line, and can be corrected in a much quicker timeframe.

The final production version is an in-line system with a conveyor feeding into and out of the camera enclosure, before feeding directly into the heat-seal machine (Figure \ref{fig:production_final}). The external cladding is modified to allow the punnet's entry and exit points without external illumination noise, or safety hazards, by constructing a protruding tunnel on both sides of the enclosure. A touchscreen was also installed to allow operators and developers easy access to the UI buttons and display. The commercially available Dell\textregistered P2418HT 24-inch capacitive touch screen can help development, as well as production, with the ability to debug whilst on the production line.


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{images/production_final.JPG}
	\caption{SQA enclosure set up in final configuration with touchscreen and cladding, set up in the Queensland facility. The SQA system feeds into the heat seal machine directly.}
	\label{fig:production_final}
\end{figure}

Conversely, the PC is installed under the out-feed conveyor and connected to the internet via a WiFi so that development can occur with a remote client. This gives developers the flexibility of being able to monitor production, develop the application, or make changes and troubleshoot without production room prescence required.




\subsection{Queensland Production - Season 1}

Due to the size and shape of the heat-seal machine and the packing bench layout, the packed punets of berries must be transported by pallet to the SQA vision system. From here they are placed on the in-feed conveyor manually, in the abscence of an automatically-fed conveyor system. A start up procedure has been documented so that the operators can start and stop the vision system when required, given that the ejector mechanism was not yet installed. However, futher work on debugging, and finalising features occured with several problems found including image storage, camera setting continuity, and thread synchronisation miss-matching. 



\subsubsection{Image Synchronisation}

Each punnet consists of four image files (one for each camera), and must be compiled and saved together for quality traceability requirements (Chapter \ref{sec:requirements}). Therefore, each image is saved into a folder accompanied by a text file detailing metadata regarding statistics of the punnet such as date/time of images and classification information. The application saves the images and text file after population occurs via the two asynchronous threads generating RGB and IR images seperately. 

However, some of the punnet's images failed to match, resulting in IR images which were not related to the RGB. When debugging the problem in the IDE it was found that if one set of images is received from the cameras and the others are not, the process would force the application to wait until the next punnet to populate the previous information. This is due to the triggered camera method, which waits for the image buffer indefinitely before proceeding, then returning to wait again some short time later. If the trigger does not occur, the thread waits until it does, before submitting images to the punnet object, leading to the conclusion that one of the punnet sensors was faulty or incorrectly calibrated. This was tested by monitoring the sensor's output via a wired connection from the sensor to the microcontroller. 

Experiments revealed that the both sensors had intermittent detection problems where they would not respond well in full-speed conditions. Trouble shooting performed included reflectance confirmation, callibration, independent power supply, and replacing sensor units, without success. As it was concluded that it was the environment creating errors (due to the metal surroundings and close proximity to objects) a redesign would be required to be refit inside the enclosure. However, a machine vision solution exists to solve this problem, given that the punnets flow in the same direction seen by a fixed camera. Optical flow is a method of image subtraction in the time domain, that is:
\begin{equation}
	I_{n+1} - I_n
\end{equation}

where $I_n$ is the image number in the stream, resulting in difference pixels (the moving objects) in an otherwise stationary scene. By analysing the difference regions, the punnet could be centred and acquired accurately, even though an extra time resource was taken up by the additional algorithms. 

\subsubsection{Ejector Mechanism}

With the vision system operational, the ejector and pneumatics were designed and added to the out-feed conveyor, as well as the attachment of the exit ramp. The ejector nozzle is positioned in the optimum location and can be moved to either side, ejecting to the left or right, providing layout flexibility. The air is supplied by the packing facility via a single $10mm$ airline which is connected to a regulator attached under the conveyor. The output of the regulator is a reduced pressure using a $6mm$ airline, given that the force of the air burst required to push the punnet would not be significant, and easier to fine tune.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{ejector.jpg}
	\caption{The pneumatic ejector system showing the yellow nozzle (top left) next to the punnet sensor and reflector, pneumatic actuator (top centre) as well as the exit ramp. \textit{Note: The regulator is not shown as it is attached underneath the conveyor.}}
	\label{fig:ejector}
\end{figure}

The punnet sensor and relector are attached as close as possible to the ejector nozzle and provides a trigger for activation of the pneumatic system, which is fired for a pre-determined period, in order to direct the punnet onto the exit ramp as smoothly as possible. Figure \ref{fig:ejector} depicts all the components (excluding the regulator) of the system shortly after testing was completed. The activation of the ejector also depends on the result of the processing of each punnet, ejecting only when defects are found. Therefore the microprocessor must know the status of each punnet so that only the identified affected punnets are ejected, whilst allowing others to pass. The microcontroller class must therefore query the results for each punnet in the array (kept as global variables in the application) when passing the ejector, in order to determine which punnets to eject and which are to remain on the production line. The microcontroller must also determine the timing of the pneumatic burst including delay between sensing and activation and the duration the solenoid is opened. 

Initial testing concluded that in order to provide enough force to push the $250g-300g$ punnet and successfully transfer it from the production conveyor (running at approximately $27cm/s$) to the exit ramp, required the following:
\begin{itemize}
	\item $90psi$ ($6bar$) of pressure
	\item $20ms$ delay time
	\item $200ms$ pneumatic duration
\end{itemize}

The image in Figure \ref{fig:ejector} shows the pneumatic nozzle offset to the exit conveyor due to the transition time of the punnet and the speed of the conveyor. This allowed for the smoothest transfer of the punnets, avoiding collision with walls or rails in order to prevent damage to sound berries.


\subsubsection{Data Collection}

A normal operating day duration consists of $10h-16h$ of constant packing with two to three meal breaks, allowing for up to $20,000$ punnets to be packed per day. However, due to fruit variability, bad weather, market conditions, and logistical reasons, the amount packed varied from $500$ to $14,000$ punnets per day over the duration of each season. Variablitiy at the beginning and end of season as well as moving operations interstate (in order to meet ideal growing condiditons) means that less fruit is packed on average as the season is beginning and concluding. The quality of the fruit also varies at these times with underripe and immature fruit seen at the beginning of each season, and overripe, mouldy, or deteriorated berries towards the end.


During the first season on the production line, a dataset of $>30,000$ punnets was acquired, with 2-4 images per punnet from each orientaion and wavelength. The system ejected 67 punnets even though the only algorithm implemented at this time was the under ripe detection. The ejector was turned on towards the end of the season, which meant that the fruit had had many months to ripen meaning a reduction in underripe instances.  

A dataset of 500+ images, ground-truthed by operators was used to assess the algorithms implemented. The current under ripe algorithm had very good results at $89.11\%$ accuracy. Examples of some typical under ripe rejections is shown in Figure \ref{fig:UR_berries}.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{UR_berries.png}
	\caption{Examples of under ripe berry detection in full punnets.}
	\label{fig:UR_berries}
\end{figure}


The Queensland season begins to harvest fruit in May/June through to October/November, with the southern farms harvesting in the opposing seasons. Therefore, in order to utilise the system constantly, it must be transported between farms (interstate) depending on the fruit volume and time of year. The system is packed and transported along with other machinery used in both farms such as the heat-seal machine, scales and metal detector. 



\subsection{South Australia Production - Season 2}


The production line in Queensland (Wamuran) differs from that of South Australia (Myponga), given the former has a manually-fed production line and the latter is automatically-fed via converging conveyor belts seen in Figure \ref{fig:myponga_line}. The packers stand either side of the large blue belt, and pack accordingly into either heat-seal punnets or non-standard stock such as second grade (fruit markets), jumbo packs ($500g$), and specialty items (A-grade, boutique, hand selected). The heat-seal punnets placed on the conveyor in line with the red in-feed conveyor of the vision system, whilst the rest are placed opposite and diverted onto a packing table. 



\begin{figure}[ht]
	\centering
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth, angle=90]{myponga_line.jpg}
		\caption{}
		\label{fig:myponga_line}
	\end{subfigure}%
	\begin{subfigure}{.35\textwidth}
		\centering
		\includegraphics[width=\linewidth]{myponga_system_line.jpg}
		\caption{}
		\label{fig:myponga_system_line}
	\end{subfigure}%
	
	\caption{(a) A view looking backwards from the enclosure down the packing line. The packing benches are seen on the left and right side of the large blue belt, with one lane for heat-seal punnets and one for jumbo/specialty punnets., (b) the heat-seal machine (foreground), SQA vision system, and packing line (background).}
	\label{}
\end{figure}

The main conveyor pictured is designed to keep running as much as possible in order to keep the packers working continuously. As the packers are paid by the number of punnets they pack, conveyor stopages result in less packing (when their immediate space on the conveyor is full) hence, loss of income for both company and workers. Therefore, the heat-seal and subsequent vision system jogging mechanism cannot be wired directly to the main conveyor, meaning that there will be an accumulation of punnets at the end, should the vision system or heat-seal machine stop. However, the main conveyor runs considerably slower than the in-feed to the vision system, resulting in taking more time to build up, and a good separation of containers as they enter the enclosure.

Two more algorithms were added to the repertoir ('overripe', and 'too small') with labelling of a test set completed. The data collection accumulated a further $70,000$ (approximately) punnets, with some lighting changes taking place during the season, such as redirecting halogen lamps upwards and blocking the top and bottom panel of the polarising tunnel. The noisy background was found to interfere with image processing, particularly when a void occurs from the top to the bottom of the punnet, creating an intense illumination from the background, within the region of interest. By blocking the top and bottom panel, this problem was solved but affected the illumination reaching the berries, and created a more dark-field lighting arrangement. Several modifications of the image processing algorithm were required, mostly in thresdholding and morphology parameters, keeping the overall method unchanged. 

Several design flaws were noticed after nearly two seasons of packing room production, such as a large build up of dirt, dust, insects, and strawberry material in the bottom of the enclosure arch, added to the dust build up on cameras, polarisers, lighting and lenses, as well as overwhelming the filters on the computer, PSU, and enclosure itself. The images shown in Figure \ref{fig:dirty_enclosure} illustrate the problems.


\begin{figure}[ht]
	\centering
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[height=0.9\linewidth,angle=270]{dirt_1.jpg}
		\caption{}
		\label{fig:dirt_1}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[height=0.9\linewidth,angle=270]{dirt_2.jpg}
		\caption{}
		\label{fig:dirt_2}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[height=0.9\linewidth,angle=270]{dirt_3.jpg}
		\caption{}
		\label{fig:dirt_3}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[height=0.9\linewidth,angle=270]{dirt_4.jpg}
		\caption{}
		\label{fig:dirt_4}
	\end{subfigure}%
	
	\caption{Enclosure particle build up showing (a) top intake fan, (b) bottom of enclosure arch with lighting removed in order to clean, (c) a filter from the enclosure full of dust, and (d) dust on the polarising sheets.}
	\label{fig:dirty_enclosure}
\end{figure}


In order to clean the enclosure properly, the bottom lighting clusters and cameras were removed to allow access to the underside of the chamber. This situation is not ideal (especially if performed each season), and along with difficulty of access to components inside the enclosure, and changed lighting arrangement, prompted a redesign of the entire system. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Colour Analysis of Strawberries on a Real-time Production Line}
\label{sec:paper_2}

\textbf{Gilbert Eaton, Andrew Busch, Rudi Bartels, and Yongsheng Gao}

\textit{Conference paper - Digital Image Computing: Techniques and Applications (DICTA) - accepted Aug 2018}


\subsection{Abstract}

\textbf{A novel system has been designed where colour analysis algorithms facilitate grading ripeness of packed strawberries on a fast-paced production line. The Strawberry quality system acquires images at the rate of $2 punnets/s$, and feeds the images to the two algorithms. Using CIELAB and HSV colourspaces, both underripe and overripe colour features are analysed  resulting in F1 scores of $94.7\%$ and $90.6\%$ respectively, when measured on multiple defect samples. The single defect class results scored $80.1\%$ and $77.1\%$. The algorithms total time for the current hardware configuration is $121ms$ maximum and $80ms$ average, which is well below the required time window of $500ms$.}
	
\textbf{$105,542$ punnets have been assessed by the algorithm and has rejected $4,952$ in total ($4.9\%$),  helping to ensure the quality of the product being shipped to customers and avoiding costly returns.}


\subsection{Background}

Real-time colour grading is an essential part of quality assessment of fruits and vegetables in order to determine ripeness or consistency, and can be used to detect skin blemishes caused by rots, mould, pests, or mishandling \cite{blasco3}. Historically, this has been performed by the people harvesting and packing them, however, the industry has recently been utilizing new technologies instead of relying on manual intervention for sorting/grading produce \cite{londhe}. Automating these processes can be very difficult to achive in agriculture, particularly in fast-paced environments where tons of produce flows from field and through to the supply chain for consumers daily. Modern advances in both computers and vision systems have allowed this type of analisis to be integrated in many production/packing lines around the world. Discussed in this article, the colour analysis method used by our vision system on a commercial strawberry packing line.

Colour analysis is commonly used as an indication of the quality of fruits and vegetables, where these features can be used to grade/sort items into categories \cite{jun, elmasry3}, to detect skin blemishes \cite{blasco3, leemans}, size and volume estimation \cite{bundit, elmasry3}, and texture analysis \cite{jun, rakun}. Multiple cameras have been used (often requiring multiple processors) to minimise uninspected surfaces \cite{zouxiao, qingzhong}, to assess multiple defects \cite{blasco4}, counting \cite{song}, 3D reconstruction \cite{panitnat} or to allow speed increases\cite{reece}.   

Other methods for fruit and vegetable quality analysis have been adopted such as infra-red image analysis or spectroscopy \cite{guthrie, bureau, yande}, and hyperspectral imaging \cite{renfu} \cite{jianwei, mendoza, rajkumar}. These acquisition systems can be used to detect defects such as internal structure estimation, soluable solids content, under-skin defects and pests, and maturity. Utilizing different wavelengths, it is a common approach to find a suitable spectral position to observe the best contrast for defects, making these features easier to extract\cite{ariana, piotr}. The system under development is intended to use a combination of colour (RGB) and infra-red (IR) bands in the future, to assess specific types of reject class such as brusing and potentially pest infestation. 

Using a conveyor system, L. Xu et al \cite{xu} achieved very good results in classifying shape, ripeness, and size of strawberries. The measurements are attained by using a K-means clustering method to find seven vertical and seven horizontal axis lines. Size feature was calculated by performing experiments to find the ratio of pixels/mm and simply dividing the pixel measurements of the berry by this ratio. Using the CIELAB colour space, a dominant colour was found in the berry by means of a histogram windowing method. Liming et al \cite{liming} used a similar approach to grade single strawberries on a conveyor in real-time. They extracted shape features by using normalised line segments on the contour with a K-means clustering method to evaluate the shape, size features by experimentally attaining the camera-object distances in terms of mm/pixel, and colour features using a CIELAB a-channel histogram windowing method to find the dominant red colour. The CIELAB colour space was also used by Lin et al \cite{lin} when they developed a strawberry calyx removal system. The single strawberries entered the vision enclosure on roller rods, before being analysed using image processing to find orientation, and finally a high pressure water jet was used to cut the green parts of the strawberry off and discarding.


The strawberry field-harvesting robot commissioned by Hayashi et al \cite{hayashi} within a greenhouse, used a method of calculating the 'Maturity Level' by analysing specific bands of the HSI colour space which represented ripe and underripe colours and intensities. As it was determined that the strawberries would be either underripe or ripe, with the event of over-ripeness ignored due to the constant operation of the harvester. They chose values of H,S, and I that equated to the colours red for ripe, whilst green, light pink, and dark pink were used for underripe in order to evaluate the ripeness before the robot picked the berries. Satoshi et al \cite{yamamoto2} also developed a robot harvester which used a red LED, green LED, and white LED to illuminate the scene in different colours before acquiring images with the same camera in order to best extract the subtle differences in shades of red and pink on the berries. 

All methods researched involved imaging single strawberries, either stationary or moving on a slow conveyor. This assumption is both inefficient and impractical given the high throughput of packing facilities and delicate flesh of the strawberry.

The proposed system in this article will perform quality grading of all fruits after they are packed in a real-time, fast-paced environment. This unique and novel vision system is designed to be capable of efficient and accurate in-line quality control for a large agricultural business.   



\subsection{Image Acquisition}

The enclosure has been designed to have a robust structure, due its placement on the production floor, and is equipped with safety systems, signals, and controls for operator use. The shell is made from $5mm$ stainless steel sheeting bolted to a $40mm$ aluminium frame. The enclosure houses all electronics, hardware and software required for sensing punnets, acquisition, and controlling the system.

The system encloses part of the production line for lighting control, using DC power and cross polarizers to stabilize the intensity \cite{eaton}. In order to capture the punnets without motion blur, the shutter speeds of the cameras must be less than $3ms$ due to the production line speed of $16.6m/min$ ($0.276m/s$). Given this shutter speed restriction, along with the added diffusers and cross-polarizers, the total illumination power required is $1200W$. The colour (BFLY-U3-23S6C-C) and mono (BFLY-U3-23S6M-C) cameras are both manufactured by Point Grey (Flir Integrated Imaging Solutions, Inc.) using the Sony IMX249 sensor with a resolution of 1920x1200 and USB3 interface. As the Quantum Efficiancy (QE) is much higher on the mono camera, it is used for NIR imaging due to the higher responses at these wavelengths.

The computer is a standard desktop PC with an ASUS P8Z68 motherboard and an 8-core i7 CPU. An Advantec PCE-USB4-00A1E four port dedicated USB3 PCI-e card is installed to ensure bandwidth is sufficient for all of the cameras. As each punnet enters the enclosure, it is detected by a photoelectric sensor which triggers the acquisition sequence. The $100W$ LED chips generate large amounts of heat due to the compact array of 100-$1W$ individual LED's. Therefore, thermal control is required to keep the LED chips from overheating. A combination of strategies has been integrated such as using $5mm$ thick stainless steel plates for heat dissipation (sink), thermal compound between LED and heatsink, and strobing the LED to reduce the duty cycle and allow cooling between images.  

\subsection{Methods}
\label{sec:colour_analysis}

The colour analysis (and all other accompanying analyses) performed must execute and complete within $500ms$ in order to assess each and every punnet. As each image is processed by up to 10 algorithms, the runtime duration of each must be reduced where possible. Although machine learning strategies such as SVM and nueral networks may improve accuracy, initial experiments showed that these classifiers require large datasets that are well labelled to perform adequate training. The time complexity of neural networks also poses a problem due to the restricted processing time \cite{he, angiulli}. 

The colour analysis described in this paper is used to distinguish under ripe and over ripe punnets. Each punnet is processed as a discrete unit, as single berries cannot be physically removed, therefore the punnet is assessed as either pass or fail in totality, with the reject berries to be replaced after inspection. This corresponds to the customer quality performance reports which indicate number of punnets rejected as opposed to number of berries. 

Market conditions, weather, supply chain, and seasonality can greatly affect the quality of crops in general, but particularly for strawberries as they are not considered a hardy fruit unlike apples and oranges. As these adversarial factors occur, the impact seen on the strawberry market can change dramatically from pricing and quality to short supply. The packing operators must be able to account for these dynamic conditions by increasing and decreasing the acceptable standard. This means that under certain circumstances, underripe, overripe, misshapen or even fruit usually considered to be too small will be packed and shipped due to market availability. Given this variation of requirements by the operators, the system must have the ability to adjust the thresholds of each quality characteristic, and therefore only reject punnets based on current market conditions or recent weather. 

In order to overcome speed concerns, and to reduce overall processing time, the colour algorithm is designed to scale each image down from a high resolution to a fraction of the original, and assess regions comprised of less pixels. This direction is, again aligned with the customer expectations due the inherrant fact that very small regions ($<3mm$) will be largely ignored by visual inspections. It is only the larger, connected regions which determine overall ripeness.

The saturation channel of the $HSV$ colourspace is used initially find the berries in each frame, due to the more colourful berries and punnet when compared to the dark background. 

To extract HSV colourspace given three channels R, G, B and $Min = min(R, G, B)$, $Max = max(R, G, B)$:
\begin{equation}
H = 
\begin{cases} 
rad(60) \times \frac{(G-B)}{Max-Min}, & R=Max \\
rad(60) \times \frac{(B-R)}{Max-Min} + 2, & G=Max \\
rad(60) \times \frac{(R-G)}{Max-Min} + 4, & B=Max \\   
\end{cases}
\end{equation}
\begin{equation}
S = 
\begin{cases} 
0, & Max=Min \\   
(Max-Min)/Max, & otherwise        
\end{cases}
\end{equation}
\begin{equation}
V = Max
\end{equation}


However, as the colour white has low saturation and can appear on underripe berries, this must be identified, extracted and concatenated to the region in order to properly calculate the underripe region areas. The red berry and white berry usually converge gradually so that the regions overlap when extracted. This means that simply taking the intersection of all white regions with known red berry regions will yield only those which are overlapping red berry as described in equations \ref{red_berry_thresh}, \ref{white_berry_thresh}, and \ref{intersect_white_berry}.

\begin{equation}
R_{red} = \sum_{i=0}^{P}t1<S_i<max(S)
\label{red_berry_thresh}
\end{equation}

\begin{equation}
R_{white} = \sum_{i=0}^{P}t2<V_i<max(V)
\label{white_berry_thresh}
\end{equation}

\begin{equation}
R_{overlap} = R_{white} \cap R_{red}
\label{intersect_white_berry}
\end{equation}

where the red and white regions ($R$) are found by using threshold values $t1$ and $t2$ over the number of pixels $P$, on the saturation ($S$) and value ($V$) channels of the transformed image, respectively. If $R_{overlap}$ is greater than zero, then $R_{white}$ is concatenated with the known red regions. 

Strawberries are member of the nonclimacteric class of fruit, meaning that ripening halts once harvested. Strawberries are more firm and will transport better when harvested just before full ripeness, although are not as full in flavour as entirely ripened fruit\cite{artur}, therefore it is acceptable for customers and realistic for pickers to expect some amount of underripeness as well as overripeness. 


\subsubsection{Underripe Features}

The ripening process, in terms of colour, for most fruit will start green and slowly transition through phases of yellow/orange, then pink before bright red and finally dark red. Figure \ref{fig:yellow_white} shows an image containing underripe berries, the yellow and white regions clearly seen.

Once the berry contour has been found and the image domain reduced to that region, underripe pixels are extracted by simply taking the difference of the $a^*$ channel and the $b^*$ channel. After contrast enhancement, areas with no red pixels are highlighted. The process is visualised in Figure \ref{fig:underripe_process}. Perfoming this analysis on the entire image results in many false positives, but this method has been found to be very effective when applied to the known berry region.   

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{zoom_image.jpg}
		\caption{}
		\label{fig:yellow_white}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{hsv_contour.jpg}
		\caption{}
		\label{fig:hsv}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{pow_image.jpg}
		\caption{}
		\label{fig:pow_image}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{result.jpg}
		\caption{}
		\label{fig:result}
	\end{subfigure}%
	
	\caption{Left to right: (a)Original image with visible white and yellow regions, (b)HSV saturation channel and berry contours after addition of white berry regions, (c)Diff(a*, b*) result highlighting absence of red colour, (d)Result of post-processing indicating underripe areas.}
	\label{fig:underripe_process}
\end{figure}


\subsubsection{Overripe Features}

\begin{figure}[ht]
	\centering
	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_berries.jpg}
		\caption{}
		\label{fig:over_berries}
	\end{subfigure}%
	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_hue.jpg}
		\caption{}
		\label{fig:over_hue}
	\end{subfigure}%
	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_diff_hue.jpg}
		\caption{}
		\label{fig:over_diff}
	\end{subfigure}%

	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_light.jpg}
		\caption{}
		\label{fig:over_light}
	\end{subfigure}%	
	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_light_diff.jpg}
		\caption{}
		\label{fig:over_light_diff}
	\end{subfigure}%
	\begin{subfigure}{.30\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{over_result.jpg}
		\caption{}
		\label{fig:over_result}
	\end{subfigure}%
	
	\caption{Left to right: (a)Berry region extracted using method from underripe algorithm, (b)Result of thresholding expanded HSV hue channel to find light colour berries, (c)Difference region of $a$ and $b$, (d)V channel used to illiminate edges. e)Intersection result of $c$ and $d$, (f)Overripe berries extracted.}
	\label{fig:overripe_process}
\end{figure} 

The colour difference between ripe and overripe berry is much narrower than the comparrison with underripe. Underripe colours may be white, green, yellow, or pink whereas overripe colour is a slightly darker red than perfectly ripened berries appear. This small variation combined with the many occlusions make the overripe features difficult to extract.

Shadowy areas of the punnet can be falsely classified as overripe berry, as the colour profiles are very similar. Therefore, the algorithm must exclude the edges of the berries in order to improve the accuracy of identifying overripe features. 

Using a known vector range in the $HSV$ colourspace $\{H_0, S_0, V_0\}, \{H_1, S_1, V_1\},....,\{H_n, S_n, V_n\}$, the good quality berry colour is  extracted and compared to the entire berry region as shown in Figure \ref{fig:over_berries}, \ref{fig:over_hue} and \ref{fig:over_diff}. Note that if there is sufficient underripe regions the algorithm will end before this point. Therfore, the vector range is inclusive of underripe colours ensuring only darker pixels are extracted in the difference of the two regions.

\begin{table*}[h]
	\caption{Results of algorithm testing.}
	\label{tab:algo_test}
	\begin{tabularx}{\textwidth}{@{}l*{8}{C}c@{}}
		\toprule
		Algorithm & Defects & FP  & FN  & TP  & TN  & Prec.(\%) & Rec.(\%) & F1 score(\%) & Y-Index(\%)\\ 
		\midrule
		Underripe   & Single   & 19 & 90 & 220 & 198 & 92.1 & 71.0 & 80.1 & 62.2 \\[6pt] 
		& Multiple & 7  & 25 & 285 & 210 & 97.6 & 91.9 & 94.7 & 88.7 \\[6pt]
		Overripe    & Single   & 44 & 82 & 212 & 177 & 82.8 & 72.1 & 77.1 & 52.2 \\[6pt]
		& Multiple & 19 & 35 & 259 & 202 & 93.2 & 88.1 & 90.6 & 79.5 \\[6pt]	 
		\bottomrule
	\end{tabularx}
\end{table*}


In order solve the problem of berry edges, an intensity threshold is applied which ignores the darker areas of the punnet including voids, berries too dark to grade, and edges where shadows occur (Fig. \ref{fig:over_light}). Performing an intersection of these regions with the dark pixels found ensures that any overripe candidate must lie on the un-occluded surfaces of berries in order to be classified properly. 
\begin{equation}
R_{cand} = R_{berry} - R_{good}
\label{diff_berry_hue}
\end{equation}

\begin{equation}
R_{norm} = \sum_{i=0}^{P}t3<V_i<max(V)
\label{berry_tops}
\end{equation}

\begin{equation}
R_{overripe} = R_{cand} \cap R_{norm}
\label{intersect_dark_berry}
\end{equation}

$R_{cand}$ is the candidate overripe region which may still contain false-positive pixels. The threshold to find the surface normal ($R_{norm}$) of the berries is then intersected with the candidates to highlight only confident matches as shown in Figures \ref{fig:over_light_diff} and \ref{fig:over_result}.


\subsection{Results}


The colour analysis algorithms have been implemented on the production line and, at the date of this paper, graded 105,542 punnets of which 4952 punnets were rejected due to failing to meet colour requirements. Continuous grading on-line will ensure that the quality of the product being packed will meet the customer requirements, and prevent reject shipments. Reject shipments can occur even when just a few poor quality punnets are observed forcing costly returns, loss of product, and reduced reputation in the industry.


Both algorithms have input sensitivity levels to help deal with seasonality. As discussed in section \ref{sec:colour_analysis}, the seasonality determines the acceptability of the fruit. This allows operators to, for example, change the sensitivity and allow slightly poorer quality fruit to pass, whilst still maintaining a threshold to remove moderate and severe cases.

Due to this variability in reject level, the quantitative analysis was split into two different measurements - single defect and multiple defects. Multiple defects meaning that there exist more than one region of the same reject class in each image. For example, a multiple defect underripe punnet will have greater than one region (that meets certain conditions) with underripe features.

Table \ref{tab:algo_test} lists the results of testing for both algorithms where the test set was ground truthed with 310 out of 527 punnets underripe, and 294 out of 515 punnets overripe in a separate set. In order to best compare the single and multiple defect classes, the same test punnets were used with the addition of more defect fruit for the multi-defect instances. The confusion matrices for the tests are detailed in Tables \ref{tab:confusion_1},  \ref{tab:confusion_2}, \ref{tab:confusion_3} and \ref{tab:confusion_4}.

The number of punnets for each test is broken into false positive, false negative, true positive, and false negative, used to calculate precision, recall, and F1-score as well as Youden's index or J statistic. Youden's index (Y-Index) is the measure of the performance of a dichotomous diagnostic test where informedness is the generalization of this method to a multi-class set.

F1-Score results for multiple defects are calculated as $94.7\%$ and $90.6\%$ for the underripe and overripe algorithms respectively. The single defect class results scored $80.1\%$ and $77.1\%$ given the high probability of occlusion and shadows, and rotational variance of each berry.


\begin{table}
	\centering
	\caption{Confusion matrix for single defect underripe tests.}
	\label{tab:confusion_1}
	\begin{tabular}{ccc}
		\toprule
		$n=527$ & Predicted: Good & Predicted: U/R  \\ 
		\midrule
		Actual: Good   & 198 & 19    \\[6pt] 
		Actual: U/R	   & 90  & 220  \\[6pt] 
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Confusion matrix for multiple defect underripe tests.}
	\label{tab:confusion_2}
	\begin{tabular}{ccc}
		\toprule
		$n=527$ & Predicted: Good & Predicted: U/R  \\ 
		\midrule
		Actual: Good   & 210 & 7    \\[6pt] 
		Actual: U/R	   & 25  & 285  \\[6pt] 
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Confusion matrix for single defect overripe tests.}
	\label{tab:confusion_3}
	\begin{tabular}{ccc}
		\toprule
		$n=515$ & Predicted: Good & Predicted: O/R  \\ 
		\midrule
		Actual: Good   & 177 & 44    \\[6pt] 
		Actual: O/R	   & 82  & 212  \\[6pt] 
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Confusion matrix for multiple defect overripe tests.}
	\label{tab:confusion_4}
	\begin{tabular}{ccc}
		\toprule
		$n=515$ & Predicted: Good & Predicted: O/R  \\ 
		\midrule
		Actual: Good   & 202 & 19    \\[6pt] 
		Actual: O/R	   & 35  & 259  \\[6pt] 
		\bottomrule
	\end{tabular}
\end{table}


Recall takes the ratio of the relevant class selected over the entire relevant class. For example, how many underripe detections were made compared to the total amount of underripe punnets. Precision measures the amount of correct detections compared to all detections. 

This indicates for both the underripe and overripe results the flase negatives are greater than the false positives. This was an intentional bias in development in order to gain maximum acceptance from operators and managers at the facility. If the system was to have low precision (even with high recall), it may be seen as more of a burden than a benifit to smooth operation of the packing line.

The total propogation time of both algorithms had a maximum of $121ms$ and average of $80ms$ when tested on a series of 100 images. The processing window is $500ms$ (time between punnets at max speed) and therefore computes and makes a decision within the required timeframe.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Conclusion}

Two colour analysis algorithms were developed to assist in grading packed strawberries in a real-time production environment. Multiple defect detection results indicate the validity of the system and it's accuracy, and operators can tune the system to suit the market conditions. However, further work is required in order to detect single defect cases more accurately. With images being acquired daily from the production line, labelling can begin in order to provide a large enough dataset to enable deep learning strategies. For example, an SVM classifier could be trained with a few hundred labelled images, but tens of thousands would be required for training a neural network which is the ultimate goal. 


The inspection system, along with these algorithms have already helped in reducing the amount of poor quality berries being shipped by the company, leading to increased Quality Assurance, Quaity Control, and mitigating potential financial losses.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Redesign and Upgrade}
\label{sec:III}


\subsection{Enclosure}

panel, encapsulation, operator friendly, 


\subsection{Lighting}

strobing, mosfets, circuits, 100W LED, 


\subsection{Microcontroller}

controllino


\subsection{Queensland Production - Season 3}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Linux and Deep Learning}
\label{sec:IV}



\subsection{Machine Learning}



\subsection{Linux Operating System}



\subsection{User Interface}



\subsection{OS Transition into Production}



\subsection{Transfer Learning with Resnet}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section{Conclusion}














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\bibliographystyle{ieeetr}
\bibliography{Master}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{SQA Specification}
\label{app:sqa_specs}


%first page
\begin{minipage}[b]{0.9\linewidth}
	\includepdf[pages=1,scale=0.8,linktodoc=true]{appendix/QC_Spec.pdf}
\end{minipage}

%rest of pages
\includepdf[pages=2-,scale=0.9]{appendix/QC_Spec.pdf}


\newpage

\section{Wiring Diagrams}
\label{app:wiring}

Version 1
%first page
\begin{minipage}[b]{0.9\linewidth}
	\includepdf[pages=1,scale=1,linktodoc=true]{appendix/wiring_1.pdf}
\end{minipage}

%rest of pages
%\includepdf[pages=2-,scale=0.9]{appendix/QC_Spec.pdf}





% that's all folks
\end{document}